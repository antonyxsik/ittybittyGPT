{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IttyBittyGPT \n",
    "\n",
    "Author: Antony Sikorski\n",
    "\n",
    "With lots of help and inspiration from Misha Ivanitsky and his LLM & Interpretability course, and Karpathy's NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we walk through:\n",
    "- Building the model\n",
    "- Setting up the dataset (we use TinyStories)\n",
    "- and training \n",
    "\n",
    "This is far from the most effective available implementation, and there are a number of things that could be improved, but I have found this to be the most effective way to learn how one works.\n",
    "\n",
    "This file has everything you need for cooking up your own little transformer and training it on a chunk of the TinyStories dataset, but doing everything in one Jupyter Notebook is not best practice. The other files in the repo are a more modular and effective way of splitting this notebook up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python_3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# necessary libraries \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from jaxtyping import Int, Float\n",
    "import tqdm\n",
    "import transformers\n",
    "import transformer_lens\n",
    "\n",
    "from muutils.misc import shorten_numerical_to_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if you have a GPU on your computer that you can run this on. That could make this process significantly faster, but you could also run out of memory (Cuda Out Of Memory error). If you don't have torch with CUDA, don't worry about this, you can just use your CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available() == True):\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use my laptop GPU, which is good news! Although I believe it only has 6 GB available, and only 1.5 of them will be available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary auto-reload for development on local machine\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "We first start off with a config class, which allows us to specify the params and size of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class GPTConfig:\n",
    "    \"\"\"Here we configure the dimensions of\n",
    "    our model. We'll set the defaults as the \n",
    "     dims from GPT2 \"\"\"\n",
    "    d_model: int = 768 # dimension of residual stream, the vectors it internally passes around \n",
    "    d_vocab: int = 50257 # defines the number of different tokens that can be represented as inputs (vocabulary size)\n",
    "    n_context: int = 1024 # maximum sequence length (context window size)\n",
    "    n_blocks: int = 12 # number of transformer blocks, frequently called n_layers but I don't like that \n",
    "    n_head: int = 12 # number of attention heads \n",
    "    head_bias: bool = True # whether to use bias in attention heads\n",
    "    mlp_expansion: int = 4 # expansion factor in MLP (they go from small to big to small, this is how many times bigger the middle layer is)\n",
    "\n",
    "    # model dimension must be divisible by number of heads\n",
    "    @property\n",
    "    def d_head(self):\n",
    "        assert self.d_model % self.n_head == 0, f\"'{self.d_model = }' must be divisible by '{self.n_head = }': {self.d_model} % {self.n_head} == {self.d_model % self.n_head}\"\n",
    "        return self.d_model // self.n_head\n",
    "    \n",
    "    @property\n",
    "    def params_shapes(self) -> dict:\n",
    "        return dict(\n",
    "            token_embeddings=(self.d_vocab, self.d_model),\n",
    "            positional_embeddings=(self.n_context, self.d_model),\n",
    "            attention_weights=(\n",
    "                self.n_blocks,\n",
    "                4,\n",
    "                self.d_model,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            attention_bias=(\n",
    "                self.n_blocks,\n",
    "                int(self.head_bias),\n",
    "                self.d_model,\n",
    "            ),\n",
    "            mlp_weights=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                self.d_model,\n",
    "                self.d_model * self.mlp_expansion,\n",
    "            ),\n",
    "            mlp_bias=(\n",
    "                self.n_blocks,\n",
    "                self.mlp_expansion + 1,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            block_layernorms=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                2,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            output_layernorm=(2, self.d_model),\n",
    "            lm_head=(self.d_model, self.d_vocab),\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def params_numel(self) -> dict:\n",
    "        return {\n",
    "            k: int(torch.tensor(v).prod())\n",
    "            for k, v in self.params_shapes.items()\n",
    "        }\n",
    "\n",
    "    # will return the total number of parameters in the model\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return sum([v for v in self.params_numel.values()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separately define the attention mechanism, which is the defining component of the transformer architecture, because it allows the model to preserve long range dependencies by learning what to pay *attention* to. \n",
    "\n",
    "It is essentially some matrix multiplications with a soft-max and a causal mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # store dimensions\n",
    "        self.n_head: int = config.n_head\n",
    "        self.d_model: int = config.d_model\n",
    "        self.n_context: int = config.n_context\n",
    "\n",
    "        # concatenating the outputs of the heads should give us d_model, but this check is done in GPTConfig\n",
    "        self.d_head: int = config.d_head\n",
    "        self.head_bias: bool = config.head_bias\n",
    "\n",
    "        # magic coefficient for scaling the dot product of the query and key in the attention calculation\n",
    "        self.sqrt_dim: float = 1.0 / math.sqrt(self.d_head)\n",
    "    \n",
    "\n",
    "        # key, query, value projections\n",
    "        self.W_K: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_Q: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_V: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        # `register_buffer` means it's not a trainable parameter\n",
    "        # the point here is to not allow the model to \"look into the future\" when making predictions\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", \n",
    "            torch.tril(\n",
    "                torch.ones(config.n_context, config.n_context)\n",
    "            )\n",
    "            .view(1, 1, config.n_context, config.n_context)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_head\"]:\n",
    "        assert x.ndim == 3, str(x.shape)\n",
    "        B, n_ctx, d_model = x.shape # batch size, sequence length, embedding dimensionality (d_model)\n",
    "        assert d_model == self.d_model, str(x.shape)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_Q(x)\n",
    "        k: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_K(x)\n",
    "        v: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_V(x)\n",
    "\n",
    "        # self-attention\n",
    "        att = (q @ k.transpose(-2, -1)) * self.sqrt_dim\n",
    "        \n",
    "        # autoregressive (causal) masking\n",
    "        att = att.masked_fill(\n",
    "            self.causal_mask[:,:n_ctx,:n_ctx] == 0, \n",
    "            float('-inf'),\n",
    "        )\n",
    "\n",
    "        # softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # apply the self-attention to the values\n",
    "        output = att @ v\n",
    "        return output.view(B, n_ctx, self.d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are matrices that need to be learned, and we should check that the dims make sense: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead(\n",
      "  (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "A: AttentionHead = AttentionHead(GPTConfig())\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual demonstration of the causal mask. Although the plot you see is a matrix of $\\{0, 1\\}$ and not $\\{-\\infty, 0\\}$, we use `masked_fill` in the  `.forward()` function to set the elements of `attn` to $-\\infty$ where $M$ is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(A.causal_mask[0, 0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the multi-headed attention component. We compute the attention heads sequentially and then concatenate them, although it is very inefficient. Practical implementations do this all at once with one matrix multiplication, but the indexing for that is difficult to read and harder to learn from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_head: int = config.n_head\n",
    "\t\tself.d_head: int = config.d_model // config.n_head\n",
    "\t\tself.d_model: int = config.d_model\n",
    "\n",
    "\t\t# attention heads from previous class\n",
    "\t\tself.attention_heads: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tAttentionHead(config) \n",
    "\t\t\tfor _ in range(self.n_head)\n",
    "\t\t])\n",
    "\n",
    "\t\t# output projection\n",
    "\t\tself.W_O: nn.Module = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\t# apply all attention heads and concatenate their outputs\n",
    "\t\t# note: in reality, you would do this all in one tensor\n",
    "\t\t# we split the attention heads up to make it easier to understand\n",
    "\t\tatt = torch.cat(\n",
    "\t\t\t[\n",
    "\t\t\t\thead(x) \n",
    "\t\t\t\tfor head in self.attention_heads\n",
    "\t\t\t],\n",
    "\t\t\tdim=-1,\n",
    "\t\t)\n",
    "\t\tassert len(att.shape) == 3, str(att.shape)\n",
    "\n",
    "\t\t# output projection\n",
    "\t\toutput = self.W_O(att)\n",
    "\t\tassert output.shape == x.shape, str(output.shape)\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer is made from transformer blocks, which include multi-headed attention, an MLP, and some LayerNorms in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# layernorm, attention, another layernorm, mlp\n",
    "\t\tself.ln_1: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.attention: nn.Module = MultiHeadedAttention(config)\n",
    "\t\tself.ln_2: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.mlp: nn.Module = nn.Sequential(\n",
    "\t\t\tnn.Linear(config.d_model, config.mlp_expansion * config.d_model),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(config.mlp_expansion * config.d_model, config.d_model),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tz = x + self.attention(self.ln_1(x))\n",
    "\t\treturn z + self.mlp(self.ln_2(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it all together to form the itty-bitty GPT. The transformer decomposes a prompt into token embeddings and positional encodings, which then go through a series of transformer blocks (number defined by `n_blocks`). This then goes through a final LayerNorm, and a linear de-embedding. \n",
    "\n",
    "What we get at the end of this process is a probability distribution over tokens, and thus we need to sample from this with some randomness (`temperature`) and convert those tokens into words to get an output. \n",
    "\n",
    "We will just use the GPT2 tokenizer for the sake of simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig, tokenizer: transformers.PreTrainedTokenizer):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.config: GPTConfig = config\n",
    "\t\tself.tokenizer: transformers.PreTrainedTokenizer = tokenizer\n",
    "\t\tassert config.d_vocab >= tokenizer.vocab_size\n",
    "\n",
    "\t\t# token and positional embeddings\n",
    "\t\tself.token_embeddings: nn.Module = nn.Embedding(config.d_vocab, config.d_model)\n",
    "\t\tself.positional_embeddings: nn.Module = nn.Embedding(config.n_context, config.d_model)\n",
    "\n",
    "\t\t# transformer\n",
    "\t\tself.transformer_blocks: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(config) \n",
    "\t\t\tfor _ in range(config.n_blocks)\n",
    "\t\t])\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tself.ln_f: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.lm_head: nn.Module = nn.Linear(config.d_model, config.d_vocab, bias=False)\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tx: Int[torch.Tensor, \"batch n_ctx\"],\n",
    "\t\t\ttargets: Int[torch.Tensor, \"batch n_ctx\"]|None = None,\n",
    "\t\t) -> tuple:\n",
    "\t\t\"\"\"returns a tuple of (logits, loss) where loss=None if targets is None\"\"\"\n",
    "\t\tassert x.ndim == 2, str(x.shape)\n",
    "\n",
    "\t\t# calculate token and positional embeddings and sum them\n",
    "\t\tx_res: Float[torch.Tensor, \"batch n_ctx d_model\"] = self.token_embeddings(x) + self.positional_embeddings(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "\t\tassert x_res.ndim == 3, str(x.shape)\n",
    "\n",
    "\t\t# transformer blocks\n",
    "\t\tfor i, block in enumerate(self.transformer_blocks):\n",
    "\t\t\tx_res = block(x_res)\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tlogits: Float[torch.Tensor, \"batch n_ctx d_vocab\"] = self.lm_head(self.ln_f(x_res))\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif targets is not None:\n",
    "\t\t\tloss = F.cross_entropy(\n",
    "\t\t\t\tlogits.transpose(1, 2),\n",
    "\t\t\t\ttargets,\n",
    "\t\t\t\tignore_index=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef generate(\n",
    "\t\tself,\n",
    "\t\tprompt: str|list[int]|Int[torch.Tensor, \"* n_ctx\"],\n",
    "\t\tmax_new_tokens: int = 128,\n",
    "\t\ttemperature: float = 1.0,\n",
    "\t) -> str:\n",
    "\n",
    "\t\t# convert prompt to string and tensor versions\n",
    "\t\tprompt_str: str\n",
    "\t\tprompt_tensor: Int[torch.Tensor, \"1 n_ctx\"]\n",
    "\t\tif isinstance(prompt, str):\n",
    "\t\t\tprompt_str = prompt\n",
    "\t\t\tprompt_tensor = torch.tensor(self.tokenizer.encode(prompt_str), dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, list):\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt)\n",
    "\t\t\tprompt_tensor = torch.tensor(prompt, dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, torch.Tensor):\n",
    "\t\t\tif prompt.ndim == 1:\n",
    "\t\t\t\tprompt = prompt.unsqueeze(0) # add batch dim\n",
    "\t\t\tassert prompt.ndim == 2\n",
    "\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt[0].tolist())\n",
    "\t\t\tprompt_tensor = prompt\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"prompt must be a string, list of ints, or PyTorch tensor\")\n",
    "\t\t\n",
    "\t\t# check tensor dims\n",
    "\t\tassert isinstance(prompt_str, str) \n",
    "\t\tassert isinstance(prompt_tensor, torch.Tensor)\n",
    "\t\tassert prompt_tensor.ndim == 2 \n",
    "\t\tassert prompt_tensor.shape[0] == 1\n",
    "\n",
    "\t\t#  device\n",
    "\t\tprompt_tensor = prompt_tensor.to(self.device)\n",
    "\n",
    "\t\t# pad the prompt if necessary\n",
    "\t\tif prompt_tensor.shape[1] < self.config.n_context:\n",
    "\t\t\tprompt_tensor = F.pad(prompt_tensor, (0, self.config.n_context - prompt_tensor.shape[1]), value=self.tokenizer.pad_token_id)\n",
    "\n",
    "\t\tassert prompt_tensor.shape[1] == self.config.n_context\n",
    "\n",
    "\t\t# iterate until max_new_tokens is reached, or an end-of-sequence token is generated\n",
    "\t\tcompletions: list[int] = list()\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\t# truncate sequence to block size\n",
    "\t\t\tprompt_len: int = prompt_tensor.shape[1]\n",
    "\t\t\tif prompt_len > self.config.n_context:\n",
    "\t\t\t\tprompt_tensor = prompt_tensor[:, -self.config.n_context:]\n",
    "\n",
    "\t\t\t# forward the model to get the logits for the index in the sequence\n",
    "\t\t\tlogits, _ = self(prompt_tensor)\n",
    "\n",
    "\t\t\t# pluck the logits at the final step and scale by desired temperature\n",
    "\t\t\tlogits = logits[:, -1, :] / temperature\n",
    "\n",
    "\t\t\t# apply softmax to convert logits to (normalized) probabilities\n",
    "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
    "\n",
    "\t\t\t# sample from the distribution\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\t\t\t# append sampled index to the running sequence and continue\n",
    "\t\t\tidx = torch.cat((prompt_tensor, idx_next), dim=1)\n",
    "\n",
    "\t\t\t# append the token to the running completions\n",
    "\t\t\tcompletions.append(int(idx_next[0, 0]))\n",
    "\n",
    "\t\t\t# check if end of sequence token is generated\n",
    "\t\t\tif idx_next == self.tokenizer.eos_token_id:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.tokenizer.decode(completions)\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\treturn sum(p.numel() for p in self.parameters())\n",
    "\t\n",
    "\t@property\n",
    "\tdef device(self) -> torch.device:\n",
    "\t\tdevice_set: set[torch.device] = set(p.device for p in self.parameters())\n",
    "\t\tassert len(device_set) == 1, device_set\n",
    "\t\treturn next(iter(device_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a full model to take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_cfg.n_params = 163009536\n",
      "test_model.n_params = 163037184\n",
      "GPT(\n",
      "  (token_embeddings): Embedding(50257, 768)\n",
      "  (positional_embeddings): Embedding(1024, 768)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_cfg = GPTConfig()\n",
    "print(f\"{test_cfg.n_params = }\")\n",
    "test_model = GPT(test_cfg, transformers.GPT2Tokenizer.from_pretrained(\"gpt2\"))\n",
    "print(f\"{test_model.n_params = }\")\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's use the TinyStories dataset, a well known dataset that gained fame when small yet still coherent models were trained on it. The dataset is made from a bunch of GPT generated children's stories, thus it does not have much diversity in content and should theoretically be pretty easy to learn. \n",
    "\n",
    "We only use a small chunk of the data for the sake of making training easy on a laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample story (story #8):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a peaceful town, there lived a little boy named Tim. Tim loved to run and play outside. One day, Tim saw a race in the park. He was excited and wanted to join the race.\\n\\nTim went to his friend, Sarah, and said, \"Let\\'s start the race!\" Sarah smiled and said, \"Yes, let\\'s go!\" They lined up with the other kids and waited for the race to begin. When they heard the word \"Go!\", they started running as fast as they could.\\n\\nTim and Sarah ran with all their speed, laughing and having fun. They could feel the wind in their hair as they raced to the finish line. In the end, Tim won the race and Sarah came in second. They were both so happy and proud of themselves. They celebrated with their friends and had a great day at the park.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grabbing the whole dataset\n",
    "text_data = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "#let's only use the training data\n",
    "text_data = text_data[\"train\"]\n",
    "\n",
    "# and let's only use the first 1000 stories \n",
    "text_data = text_data[:100]\n",
    "\n",
    "#what does a story look like? \n",
    "print(\"\\n Sample story (story #8):\")\n",
    "text_data['text'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's turn our dataset into a big long list of strings, and check how long it is (we want this to be small, millions or less): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76060"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = \"\\n\\n\".join(text_data['text'])\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: If you ever think this code isn't working, and want to do a sanity check, train the model on this dataset (which is just the letter \"a\" a bunch of times). When prompted with \"a\", your model output should feature tons of \"a\"s. You can use this as a sanity check (purely hypothetical, I definitely didn't do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a text dataset like text_data but filled strictly with \"a\" characters\n",
    "# text_data_a = \"a\" * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our TextDataset class, which is slightly inefficient because we have a fixed context size which we will stick with, and then we will trim off any data that isn't divisible by the context size. Ex: if we had 110 items in our dataset, and the context window is 25, it will chop off the last 10. Again, far from the best way, but it works and that's what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\ttext: str, \n",
    "\t\t\ttokenizer: transformers.PreTrainedTokenizer,\n",
    "\t\t\tn_context: int,\n",
    "\t\t\tensure_n_context_match: bool = True,\n",
    "\t\t):\n",
    "\t\t# add 1 to n_context to account for the target token\n",
    "\t\tn_context += 1\n",
    "\n",
    "\t\t# tokenize the text\n",
    "\t\ttokenized_text: list[int] = tokenizer.encode(text)\n",
    "\t\tself.total_tokens: int = len(tokenized_text)\n",
    "\n",
    "\t\t# trim the last tokens to make sure the length is a multiple of n_context\n",
    "\t\tif ensure_n_context_match:\n",
    "\t\t\ttokenized_text = tokenized_text[:-(len(tokenized_text) % n_context)]\n",
    "\t\t\tself.total_tokens = len(tokenized_text)\n",
    "\n",
    "\t\t# split the text into examples of length n_context\n",
    "\t\t# this means that text will often start in the middle of a sentence\n",
    "\t\t# in reality, we might want to do this a bit smarter\n",
    "\t\tself.examples: list[list[int]] = [\n",
    "\t\t\ttokenized_text[i:i+n_context] \n",
    "\t\t\tfor i in range(0, len(tokenized_text), n_context)\n",
    "\t\t]\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.examples)\n",
    "\t\n",
    "\tdef __getitem__(self, i: int) -> Float[torch.Tensor, \"n_ctx\"]:\n",
    "\t\treturn torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\t\n",
    "\tdef example_lengths(self) -> Counter[int]:\n",
    "\t\treturn Counter(len(ex) for ex in self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Here we train the model! First, we define our training loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\tmodel: GPT,\n",
    "\ttext: str,\n",
    "\toptimizer: torch.optim.Optimizer,\n",
    "\tscheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "\tdevice: torch.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size: int = 8,\n",
    "\tmax_batches: int|None = None,\n",
    "\tprint_interval: int = 100,\n",
    "\tepochs: int = 1,\n",
    ") -> tuple[GPT, list[dict]]:\n",
    "\t\n",
    "\t# move model to device\n",
    "\tprint(f\"moving model to device: {device}\")\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t# set up data\n",
    "\tprint(f\"setting up dataset from text of length {len(text)}\")\n",
    "\tdataset: TextDataset = TextDataset(\n",
    "\t\ttext=text, \n",
    "\t\ttokenizer=model.tokenizer, \n",
    "\t\tn_context=model.config.n_context,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataset with {len(dataset)} examples, example lengths: {dataset.example_lengths()}\")\n",
    "\n",
    "\tprint(f\"setting up dataloader from {len(dataset)} examples\")\n",
    "\tdataloader: DataLoader = DataLoader(\n",
    "\t\tdataset, \n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tshuffle=True,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataloader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "\t# set up training loop\n",
    "\tprint(\"training...\")\n",
    "\ttraining_records: list[dict] = list()\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}\\n\")\n",
    "\t\ti: int; batch: Float[torch.Tensor, \"batch n_ctx\"]\n",
    "\t\tfor i, batch in tqdm.tqdm(\n",
    "\t\t\tenumerate(dataloader),\n",
    "\t\t\ttotal=len(dataloader),\n",
    "\t\t\tdesc=\"Training\",\n",
    "\t\t):\n",
    "\t\t\t# move batch to device\n",
    "\t\t\tbatch = batch.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# break if we've reached the maximum number of batches\n",
    "\t\t\tif max_batches is not None and i > max_batches:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# forward pass\n",
    "\t\t\tlogits, loss = model(\n",
    "\t\t\t\tbatch[:, :-1],\n",
    "\t\t\t\ttargets=batch[:, 1:], # the targets are just the input, offset by one\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# backward pass\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# record progress\n",
    "\t\t\ttraining_records.append({\n",
    "\t\t\t\t\"batch\": i,\n",
    "\t\t\t\t\"loss\": loss.item(),\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\tif i % print_interval == 0:\n",
    "\t\t\t\tprint(f\"Batch {i}, Loss: {loss.item()}\\n\")\n",
    "\n",
    "\t\tscheduler.step()\n",
    "    \t#print(f\"Updated learning rate to: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "\treturn model, training_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure (define params for) our own model (which will be tiny) and do some setup before we train it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER.vocab_size = 50257 \n",
      "\n",
      "Muutils rounded model params: \n",
      "MODEL.n_params = 807K \n",
      "\n",
      "Full model params: \n",
      "MODEL.n_params = 806896\n"
     ]
    }
   ],
   "source": [
    "# using the GPT2 tokenizer, and making sure it has the same vocab size as the model\n",
    "TOKENIZER: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"{TOKENIZER.vocab_size = } \\n\")\n",
    "\n",
    "\n",
    "# set up a config for a small model\n",
    "CONFIG: GPTConfig = GPTConfig(\n",
    "\td_model=8,\n",
    "\td_vocab=50257,\n",
    "\tn_context=128,\n",
    "\tn_blocks=2,\n",
    "\tn_head=4,\n",
    ")\n",
    "\n",
    "# not the most necessary check but it felt good to do\n",
    "assert(TOKENIZER.vocab_size == GPTConfig().d_vocab)\n",
    "\n",
    "# initialize the model\n",
    "MODEL: GPT = GPT(CONFIG, TOKENIZER)\n",
    "\n",
    "#two ways of printing number of model params\n",
    "print(\"Muutils rounded model params: \")\n",
    "print(f\"MODEL.n_params = {shorten_numerical_to_str(MODEL.n_params)} \\n\")\n",
    "print(\"Full model params: \")\n",
    "print(f\"MODEL.n_params = {MODEL.n_params}\")\n",
    "\n",
    "# choice of optimizer\n",
    "OPTIMIZER: torch.optim.Optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-1)\n",
    "#OPTIMIZER: torch.optim.Optimizer = torch.optim.SGD(MODEL.parameters(), lr=1e-1)\n",
    "# Initialize the learning rate scheduler\n",
    "SCHEDULER: StepLR = StepLR(OPTIMIZER, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving model to device: cuda"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18985 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "setting up dataset from text of length 76060\n",
      "\tset up dataset with 147 examples, example lengths: Counter({129: 147})\n",
      "setting up dataloader from 147 examples\n",
      "\tset up dataloader with 5 batches of size 32\n",
      "training...\n",
      "Epoch 1/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:02<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 10.996191024780273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:02<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.412898540496826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.077994346618652\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.8305253982543945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.740299224853516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.732051372528076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.609669208526611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.586735725402832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.414438247680664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.275589466094971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.175333023071289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.076552391052246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.960487365722656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.794473171234131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.612510681152344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.456562519073486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.2682695388793945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.247232437133789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.007120132446289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.919843912124634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.828779935836792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.771991014480591\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.679422616958618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.7336418628692627\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.666445732116699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.6640830039978027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.5375072956085205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.56088924407959\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4465444087982178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4558656215667725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.354921579360962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4253392219543457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.301480531692505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.388551950454712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2704405784606934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3785653114318848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.289022445678711\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3116445541381836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.333580732345581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3421921730041504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.300762176513672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.31797456741333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3032357692718506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.252319574356079\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3085992336273193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.312325954437256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.176494836807251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2706167697906494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2670674324035645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2813639640808105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3265271186828613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2574334144592285\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3307693004608154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.290410280227661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2459568977355957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2642757892608643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.313551902770996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2170932292938232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2072784900665283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.263341188430786\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2539002895355225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.262500047683716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.250119686126709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.211700677871704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2415099143981934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2066831588745117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2315480709075928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2262871265411377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2426071166992188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1837775707244873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2898035049438477\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1805989742279053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.226116180419922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.213773727416992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.182772397994995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2330636978149414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.266331434249878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2715206146240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:00<00:00,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1940135955810547\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.230990171432495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1858527660369873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2637217044830322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1614372730255127\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:01<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2729625701904297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.23771595954895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2109901905059814\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2800230979919434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2131099700927734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.207932710647583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:04<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1989264488220215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2044517993927\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.275451898574829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1337411403656006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2047863006591797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2032902240753174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:01,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.19120717048645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.237370252609253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.161142587661743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:01,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2532641887664795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2396087646484375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1478664875030518\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.186497449874878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:01<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:01<00:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.244889974594116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1625964641571045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.264606475830078\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:01<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.250946044921875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2210476398468018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1989755630493164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:02<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:01,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2543392181396484\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:00<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.210721731185913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.298192024230957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.219848871231079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:00<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1833271980285645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2257080078125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:01,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.220615863800049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.234238386154175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:00<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.196181058883667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.226818799972534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:01<00:04,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1815834045410156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [00:00<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2873728275299072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:00<00:00,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINED, training_history = train(\n",
    "\tmodel=MODEL,\n",
    "\ttext=text_data,\n",
    "\toptimizer=OPTIMIZER,\n",
    "    scheduler = SCHEDULER,\n",
    "\tdevice=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size=32,\n",
    "\tmax_batches=None,\n",
    "\tprint_interval=100,\n",
    "\tepochs=120,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save our trained model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MODEL_TRAINED, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code for loading the model back in: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model\n",
    "\n",
    "First, let's take a quick look at our loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVxklEQVR4nO3deVhUZf8G8HuYgWEf9k3ZBBQVFxQ1c09yyczMNtNS29PesvXVSjN7jcz2TSv7qalpZWkuKS65ZC6IgrsgArLIIiLMsA0wc35/IAdGQJEBzgzen+ua62LOMvOdEzG3z3kWmSAIAoiIiIjMlIXUBRAREREZg2GGiIiIzBrDDBEREZk1hhkiIiIyawwzREREZNYYZoiIiMisMcwQERGRWWOYISIiIrPGMENERERmjWGGiJrV1KlTERAQ0KRz582bB5lM1rwFEVGbxzBDdJuQyWSNeuzZs0fqUiUxdepU2NvbS10GETWBjGszEd0eVq1aZfD8p59+wo4dO7By5UqD7XfffTc8PT2b/D4VFRXQ6/VQKpW3fG5lZSUqKythbW3d5PdvqqlTp2LdunUoKipq9fcmIuMopC6AiFrH5MmTDZ4fOnQIO3bsqLP9eiUlJbC1tW30+1haWjapPgBQKBRQKPhniYhuDW8zEZFo6NChCAsLw9GjRzF48GDY2trirbfeAgD8+eefGDNmDHx8fKBUKhEUFIT3338fOp3O4DWu7zOTmpoKmUyGjz/+GN9//z2CgoKgVCrRp08fHDlyxODc+vrMyGQyvPjii9iwYQPCwsKgVCrRtWtXbNu2rU79e/bsQUREBKytrREUFITvvvuu2fvh/Pbbb+jduzdsbGzg5uaGyZMnIzMz0+CY7OxsTJs2De3bt4dSqYS3tzfGjRuH1NRU8ZjY2FiMHDkSbm5usLGxQWBgIJ588slmq5PodsJ/AhGRgStXrmD06NF49NFHMXnyZPGW0/Lly2Fvb49XX30V9vb2+PvvvzF37lyo1WosWrTopq/7888/Q6PR4LnnnoNMJsNHH32EBx54AMnJyTdtzdm/fz/++OMPTJ8+HQ4ODvjyyy8xYcIEpKWlwdXVFQAQFxeHUaNGwdvbG++99x50Oh3mz58Pd3d34y/KNcuXL8e0adPQp08fREVFIScnB1988QX+/fdfxMXFwcnJCQAwYcIEnD59Gv/5z38QEBCA3Nxc7NixA2lpaeLzESNGwN3dHbNmzYKTkxNSU1Pxxx9/NFutRLcVgYhuSzNmzBCu/xMwZMgQAYCwZMmSOseXlJTU2fbcc88Jtra2QllZmbhtypQpgr+/v/g8JSVFACC4uroK+fn54vY///xTACBs2rRJ3Pbuu+/WqQmAYGVlJSQlJYnbjh8/LgAQvvrqK3Hb2LFjBVtbWyEzM1Pcdv78eUGhUNR5zfpMmTJFsLOza3B/eXm54OHhIYSFhQmlpaXi9s2bNwsAhLlz5wqCIAhXr14VAAiLFi1q8LXWr18vABCOHDly07qI6OZ4m4mIDCiVSkybNq3OdhsbG/FnjUaDvLw8DBo0CCUlJTh37txNX/eRRx6Bs7Oz+HzQoEEAgOTk5JueGxkZiaCgIPF59+7d4ejoKJ6r0+mwc+dO3H///fDx8RGPCw4OxujRo2/6+o0RGxuL3NxcTJ8+3aCD8pgxYxAaGootW7YAqLpOVlZW2LNnD65evVrva1W34GzevBkVFRXNUh/R7YxhhogMtGvXDlZWVnW2nz59GuPHj4dKpYKjoyPc3d3FzsOFhYU3fV0/Pz+D59XBpqEv/BudW31+9bm5ubkoLS1FcHBwnePq29YUFy9eBAB06tSpzr7Q0FBxv1KpxMKFC7F161Z4enpi8ODB+Oijj5CdnS0eP2TIEEyYMAHvvfce3NzcMG7cOCxbtgxarbZZaiW63TDMEJGB2i0w1QoKCjBkyBAcP34c8+fPx6ZNm7Bjxw4sXLgQAKDX62/6unK5vN7tQiNmhzDmXCnMnDkTiYmJiIqKgrW1NebMmYPOnTsjLi4OQFWn5nXr1uHgwYN48cUXkZmZiSeffBK9e/fm0HCiJmCYIaKb2rNnD65cuYLly5fj5Zdfxr333ovIyEiD20ZS8vDwgLW1NZKSkursq29bU/j7+wMAEhIS6uxLSEgQ91cLCgrCa6+9hu3bt+PUqVMoLy/HJ598YnDMHXfcgQULFiA2NharV6/G6dOnsXbt2mapl+h2wjBDRDdV3TJSuyWkvLwc3377rVQlGZDL5YiMjMSGDRtw6dIlcXtSUhK2bt3aLO8REREBDw8PLFmyxOB20NatW3H27FmMGTMGQNW8PGVlZQbnBgUFwcHBQTzv6tWrdVqVevbsCQC81UTUBByaTUQ3deedd8LZ2RlTpkzBSy+9BJlMhpUrV5rUbZ558+Zh+/btGDBgAF544QXodDp8/fXXCAsLQ3x8fKNeo6KiAv/73//qbHdxccH06dOxcOFCTJs2DUOGDMHEiRPFodkBAQF45ZVXAACJiYkYPnw4Hn74YXTp0gUKhQLr169HTk4OHn30UQDAihUr8O2332L8+PEICgqCRqPBDz/8AEdHR9xzzz3Ndk2IbhcMM0R0U66urti8eTNee+01vPPOO3B2dsbkyZMxfPhwjBw5UuryAAC9e/fG1q1b8frrr2POnDnw9fXF/Pnzcfbs2UaNtgKqWpvmzJlTZ3tQUBCmT5+OqVOnwtbWFh9++CH++9//ws7ODuPHj8fChQvFEUq+vr6YOHEidu3ahZUrV0KhUCA0NBS//vorJkyYAKCqA3BMTAzWrl2LnJwcqFQq9O3bF6tXr0ZgYGCzXROi2wXXZiKiNu3+++/H6dOncf78ealLIaIWwj4zRNRmlJaWGjw/f/48/vrrLwwdOlSagoioVbBlhojaDG9vb0ydOhUdOnTAxYsXsXjxYmi1WsTFxSEkJETq8oiohbDPDBG1GaNGjcKaNWuQnZ0NpVKJ/v3744MPPmCQIWrj2DJDREREZo19ZoiIiMisMcwQERGRWWvzfWb0ej0uXboEBwcHyGQyqcshIiKiRhAEARqNBj4+PrCwuHHbS5sPM5cuXYKvr6/UZRAREVETpKeno3379jc8ps2HGQcHBwBVF8PR0VHiaoiIiKgx1Go1fH19xe/xG2nzYab61pKjoyPDDBERkZlpTBcRdgAmIiIis8YwQ0RERGaNYYaIiIjMGsMMERERmTWGGSIiIjJrDDNERERk1hhmiIiIyKwxzBAREZFZY5ghIiIisyZpmNm3bx/Gjh0LHx8fyGQybNiwwWD/H3/8gREjRsDV1RUymQzx8fGS1ElERESmS9IwU1xcjB49euCbb75pcP/AgQOxcOHCVq6MiIiIzIWkazONHj0ao0ePbnD/448/DgBITU1tpYqIiIjI3LT5hSZbSkl5JfKLy6FUyOHuoJS6HCIiottWm+sArNVqoVarDR4t4ft9yRi4cDc+3ZHYIq9PREREjdPmwkxUVBRUKpX48PX1bZH3sVdWNWqVlFe2yOsTERFR47S5MDN79mwUFhaKj/T09BZ5H7trYaZYyzBDREQkpTbXZ0apVEKpbPk+LNVhpohhhoiISFKShpmioiIkJSWJz1NSUhAfHw8XFxf4+fkhPz8faWlpuHTpEgAgISEBAODl5QUvLy9Jaq5mr5QDAIq1OknrICIiut1JepspNjYW4eHhCA8PBwC8+uqrCA8Px9y5cwEAGzduRHh4OMaMGQMAePTRRxEeHo4lS5ZIVnM1OyveZiIiIjIFkrbMDB06FIIgNLh/6tSpmDp1ausVdAt4m4mIiMg0tLkOwK3Fnh2AiYiITALDTBOJo5nKddDrG25dIiIiopbFMNNE1S0zAFBSwU7AREREUmGYaSJrSwtYyKp+LuGtJiIiIskwzDSRTCYTRzSxEzAREZF0GGaMUDMLMG8zERERSYVhxgh21ybOY8sMERGRdBhmjMDh2URERNJjmDFCzfBshhkiIiKpMMwYQamounzllXqJKyEiIrp9McwYQW5Rdfl0nDSPiIhIMgwzRlBcm2imkmGGiIhIMgwzRpDLq8IMW2aIiIikwzBjBLmMLTNERERSY5gxQvVtJp2eHYCJiIikwjBjBDn7zBAREUmOYcYIiuo+MzqGGSIiIqkwzBihumVGJzDMEBERSYVhxggKzjNDREQkOYYZI7DPDBERkfQYZoxQM5qJYYaIiEgqDDNGEFtm2AGYiIhIMgwzRpBznhkiIiLJMcwYgX1miIiIpMcwYwT2mSEiIpIew4wR5NeGZrNlhoiISDoMM0ZgywwREZH0GGaMIGeYISIikhzDjBHEtZkYZoiIiCTDMGOEmtFMHJpNREQkFYYZI7DPDBERkfQYZozA0UxERETSkzTM7Nu3D2PHjoWPjw9kMhk2bNhgsF8QBMydOxfe3t6wsbFBZGQkzp8/L02x9ZBfu3psmSEiIpKOpGGmuLgYPXr0wDfffFPv/o8++ghffvkllixZgsOHD8POzg4jR45EWVlZK1daP7FlhmszERERSUYh5ZuPHj0ao0ePrnefIAj4/PPP8c4772DcuHEAgJ9++gmenp7YsGEDHn300dYstV7sM0NERCQ9k+0zk5KSguzsbERGRorbVCoV+vXrh4MHDzZ4nlarhVqtNni0FI5mIiIikp7Jhpns7GwAgKenp8F2T09PcV99oqKioFKpxIevr2+L1ciWGSIiIumZbJhpqtmzZ6OwsFB8pKent9h7cdVsIiIi6ZlsmPHy8gIA5OTkGGzPyckR99VHqVTC0dHR4NFSFNc6ALNlhoiISDomG2YCAwPh5eWFXbt2idvUajUOHz6M/v37S1hZDa7NREREJD1JRzMVFRUhKSlJfJ6SkoL4+Hi4uLjAz88PM2fOxP/+9z+EhIQgMDAQc+bMgY+PD+6//37piq6FazMRERFJT9IwExsbi2HDhonPX331VQDAlClTsHz5crz55psoLi7Gs88+i4KCAgwcOBDbtm2DtbW1VCUbsJCxzwwREZHUJA0zQ4cOhSA0HARkMhnmz5+P+fPnt2JVjcfRTERERNIz2T4z5oDzzBAREUmPYcYI7DNDREQkPYYZIyg4zwwREZHkGGaMUL3QpI4LTRIREUmGYcYIbJkhIiKSHsOMEcRJ824wIouIiIhaFsOMETg0m4iISHoMM0aovZzBjebLISIiopbDMGOE6jADsHWGiIhIKgwzRqgdZtgJmIiISBoMM0ZQWNRcPrbMEBERSYNhxghsmSEiIpIew4wRFOwzQ0REJDmGGSNYWMggu5ZnuNgkERGRNBhmjMS5ZoiIiKTFMGMkOcMMERGRpBhmjFQ9oolhhoiISBoMM0ayEPvMMMwQERFJgWHGSAo5W2aIiIikxDBjpOo+M5U6hhkiIiIpMMwYiaOZiIiIpMUwYySxZYbzzBAREUmCYcZIbJkhIiKSFsOMkWpaZhhmiIiIpMAwYyTOM0NERCQthhkjcQZgIiIiaTHMGIlhhoiISFoMM0ZinxkiIiJpMcwYqWY0E4dmExERSYFhxkhsmSEiIpIWw4yRFHL2mSEiIpISw4yR5NeGZnNtJiIiImmYfJjRaDSYOXMm/P39YWNjgzvvvBNHjhyRuiwRZwAmIiKSlsmHmaeffho7duzAypUrcfLkSYwYMQKRkZHIzMyUujQA7DNDREQkNZMOM6Wlpfj999/x0UcfYfDgwQgODsa8efMQHByMxYsXS10eAI5mIiIikppC6gJupLKyEjqdDtbW1gbbbWxssH///nrP0Wq10Gq14nO1Wt2iNbJlhoiISFom3TLj4OCA/v374/3338elS5eg0+mwatUqHDx4EFlZWfWeExUVBZVKJT58fX1btEbOAExERCQtkw4zALBy5UoIgoB27dpBqVTiyy+/xMSJE2FhUX/ps2fPRmFhofhIT09v0foYZoiIiKRl0reZACAoKAh79+5FcXEx1Go1vL298cgjj6BDhw71Hq9UKqFUKlutPgVvMxEREUnK5FtmqtnZ2cHb2xtXr15FdHQ0xo0bJ3VJAGrmmWHLDBERkTRMvmUmOjoagiCgU6dOSEpKwhtvvIHQ0FBMmzZN6tIAsGWGiIhIaibfMlNYWIgZM2YgNDQUTzzxBAYOHIjo6GhYWlpKXRqA2n1mODSbiIhICibfMvPwww/j4YcflrqMBrFlhoiISFom3zJj6uTVC01ybSYiIiJJMMwYiS0zRERE0mKYMZJcxnlmiIiIpMQwYyRxaLbAMENERCQFhhkjKdhnhoiISFIMM0biQpNERETSYpgxkoLzzBAREUmKYcZIbJkhIiKSFsOMkRRcNZuIiEhSDDNGqh7NxJYZIiIiaTDMGIktM0RERNJimDES+8wQERFJi2HGSFw1m4iISFoMM0aS8zYTERGRpBhmjMQ+M0RERNJimDES+8wQERFJi2HGSOLaTAwzREREkmCYMZI4zwwXmiQiIpIEw4yR2GeGiIhIWgwzRqrpM8Oh2URERFJgmDESW2aIiIikxTBjpOqWmQr2mSEiIpIEw4yRFNc6ALNlhoiISBoMM0biPDNERETSYpgxkqWcHYCJiIikxDBjJIWc88wQERFJiWHGSAqxAzBbZoiIiKTAMGMkLmdAREQkLYYZI1WPZqrUCxAEBhoiIqLWxjBjpOoOwABHNBEREUmBYcZI1R2AAXYCJiIikoJJhxmdToc5c+YgMDAQNjY2CAoKwvvvv29St3OqOwADHJ5NREQkBYXUBdzIwoULsXjxYqxYsQJdu3ZFbGwspk2bBpVKhZdeeknq8gBcF2bYMkNERNTqTDrMHDhwAOPGjcOYMWMAAAEBAVizZg1iYmIkrqyGvFaYqWDLDBERUasz6dtMd955J3bt2oXExEQAwPHjx7F//36MHj26wXO0Wi3UarXBoyXJZLKaWYDZMkNERNTqTLplZtasWVCr1QgNDYVcLodOp8OCBQswadKkBs+JiorCe++914pVVg3PrtDpONcMERGRBEy6ZebXX3/F6tWr8fPPP+PYsWNYsWIFPv74Y6xYsaLBc2bPno3CwkLxkZ6e3uJ1chZgIiIi6Zh0y8wbb7yBWbNm4dFHHwUAdOvWDRcvXkRUVBSmTJlS7zlKpRJKpbI1yxRnAeY8M0RERK3PpFtmSkpKYGFhWKJcLofexDraVs81w5YZIiKi1mfSLTNjx47FggUL4Ofnh65duyIuLg6ffvopnnzySalLM2BpwfWZiIiIpGLSYearr77CnDlzMH36dOTm5sLHxwfPPfcc5s6dK3VpBuTy6j4zDDNEREStzaTDjIODAz7//HN8/vnnUpdyQ5bVi03yNhMREVGrM+k+M+aCHYCJiIikwzDTDBTVLTMMM0RERK2OYaYZiC0zvM1ERETU6hhmmkHNpHlsmSEiImptDDPNoHqemUoTm/+GiIjodsAw0wyqF5rkPDNEREStj2GmGcgtqmcAZpghIiJqbQwzzaB6BmB2ACYiImp9DDPNoHo0UwVvMxEREbU6hplmUN0BWMeWGSIiolbHMNMMqodmc9I8IiKi1scw0wwU7ABMREQkGYaZZmDJGYCJiIgkwzDTDLjQJBERkXQYZppBzUKTbJkhIiJqbQwzzUDsAMw+M0RERK2OYaYZVA/NZgdgIiKi1scw0wxq1mbibSYiIqLWxjDTDMSh2ewATERE1OoYZpqBgkOziYiIJMMw0wzYAZiIiEg6DDPNoLoDMOeZISIian1NCjPp6enIyMgQn8fExGDmzJn4/vvvm60wcyLOAMwOwERERK2uSWHmsccew+7duwEA2dnZuPvuuxETE4O3334b8+fPb9YCzYH82m0mDs0mIiJqfU0KM6dOnULfvn0BAL/++ivCwsJw4MABrF69GsuXL2/O+syCZfUMwOwATERE1OqaFGYqKiqgVCoBADt37sR9990HAAgNDUVWVlbzVWcmuDYTERGRdJoUZrp27YolS5bgn3/+wY4dOzBq1CgAwKVLl+Dq6tqsBZoDsQMwbzMRERG1uiaFmYULF+K7777D0KFDMXHiRPTo0QMAsHHjRvH20+1EHJrNDsBEREStTtGUk4YOHYq8vDyo1Wo4OzuL25999lnY2to2W3HmQsEOwERERJJpUstMaWkptFqtGGQuXryIzz//HAkJCfDw8GjWAs2B5bXbTDr2mSEiImp1TQoz48aNw08//QQAKCgoQL9+/fDJJ5/g/vvvx+LFi5u1QHNQ3QG4gqOZiIiIWl2TwsyxY8cwaNAgAMC6devg6emJixcv4qeffsKXX37ZrAWaA7kFRzMRERFJpUlhpqSkBA4ODgCA7du344EHHoCFhQXuuOMOXLx4sVkLDAgIgEwmq/OYMWNGs76PMSzlnGeGiIhIKk0KM8HBwdiwYQPS09MRHR2NESNGAAByc3Ph6OjYrAUeOXIEWVlZ4mPHjh0AgIceeqhZ38cYCrbMEBERSaZJYWbu3Ll4/fXXERAQgL59+6J///4AqlppwsPDm7VAd3d3eHl5iY/NmzcjKCgIQ4YMadb3MYYl55khIiKSTJOGZj/44IMYOHAgsrKyxDlmAGD48OEYP358sxV3vfLycqxatQqvvvoqZDJZvcdotVpotVrxuVqtbrF6qsk5zwwREZFkmhRmAIgtJdWrZ7dv377FJ8zbsGEDCgoKMHXq1AaPiYqKwnvvvdeidVzPUs55ZoiIiKTSpNtMer0e8+fPh0qlgr+/P/z9/eHk5IT3338f+hZsnfjxxx8xevRo+Pj4NHjM7NmzUVhYKD7S09NbrJ5qCgvOM0NERCSVJrXMvP322/jxxx/x4YcfYsCAAQCA/fv3Y968eSgrK8OCBQuatUigamK+nTt34o8//rjhcUqlUlwEs7VwnhkiIiLpNCnMrFixAkuXLhVXywaA7t27o127dpg+fXqLhJlly5bBw8MDY8aMafbXNlZ1ywxHMxEREbW+Jt1mys/PR2hoaJ3toaGhyM/PN7qo6+n1eixbtgxTpkyBQtHkbj4tprplRqcXIAgMNERERK2pSWGmR48e+Prrr+ts//rrr9G9e3eji7rezp07kZaWhieffLLZX7s5WFrUXEa2zhAREbWuJjVzfPTRRxgzZgx27twpzjFz8OBBpKen46+//mrWAgFgxIgRJt3iUd0yA1TNNWMpl7AYIiKi20yTWmaGDBmCxMREjB8/HgUFBSgoKMADDzyA06dPY+XKlc1do8mrHWYqONcMERFRq5IJzdjkcfz4cfTq1Qs6na65XtJoarUaKpUKhYWFzb7UQjWdXkDQW1UtUsfm3A0XO6sWeR8iIqLbxa18fzepZYYMyS1kqJ6QuLGzABdpK1uwIiIiotsHw0wzqe4E3Jj1mfYlXkbYu9GI2nq2pcsiIiJq8xhmmkl1v5nGhJn5m88AAL7bm9yiNREREd0Obmk00wMPPHDD/QUFBcbUYtaqF5tkB2AiIqLWdUthRqVS3XT/E088YVRB5spS3vj1mUx5mDkREZG5uaUws2zZspaqw+wpLBq/PhOjDBERUfNhn5lmUt0y05g+M0RERNR8GGaaSXWfGS5nQERE1LoYZppJzWimRnQAZt4hIiJqNgwzzUScZ4YtM0RERK2KYaaZVLfMsAMwERFR62KYaSbVo5k4NJuIiKh1Mcw0E8W10UwVHM1ERETUqhhmmolCHM3EGYCJiIhaE8NMM7mVeWbYdkNERNR8GGaaifwWZgAmIiKi5sMw00ysLasupbayEaOZ2DRDRETUbBhmmomNpRwAUFahu+mxAm80ERERNRuGmWZiY1UVZkrLddBW6pCUq5G4IiIiotsDw0wzsb7WMlNaocN/fo5D5Kf7cOBCXr3H8jYTERFR82GYaSbVt5k0ZZXYfiYHALAmJl3KkoiIiG4LDDPNpDrM/JtU0xrT3tlGqnKIiIhuGwwzzaS6z0xyXrG4rbC0ot5ja99m4tIGRERExmGYaSbVYaa2wpL6w0xtXGWbiIjIOAwzzaT6NlNtV0vKb3peYxamJCIiooYxzDST+sNMQ7eZagIMW2aIiIiMwzDTTKzruc1U0EDLTO34ouMq20REREZhmGkm9bXMFDSqzwzXciIiIjIGw0wzqS/MlFbo6l3eoPatJfaZISIiMo7Jh5nMzExMnjwZrq6usLGxQbdu3RAbGyt1WXXUN5oJqL91prLWytrsM0NERGQchdQF3MjVq1cxYMAADBs2DFu3boW7uzvOnz8PZ2dnqUuro3bLjMrGEvZKBTILSpGSVwwvlbXBsbUDTCX7zBARERnFpMPMwoUL4evri2XLlonbAgMDJayoYda1wkw7Jxv4utggs6AUpzILcUcHF+gFQG4hA2AYYNhnhoiIyDgmfZtp48aNiIiIwEMPPQQPDw+Eh4fjhx9+uOE5Wq0WarXa4NEabGvdZgr2sEe3dioAwMnMQjzzUyyGLNqNkvJKAIYBhn1miIiIjGPSYSY5ORmLFy9GSEgIoqOj8cILL+Cll17CihUrGjwnKioKKpVKfPj6+rZKrbVbZjq426HrtTCz8fgl7Dybi4yrpTh2sQDAdbeZGGaIiIiMYtJhRq/Xo1evXvjggw8QHh6OZ599Fs888wyWLFnS4DmzZ89GYWGh+EhPb52Vq6tvIQFAR08H9O/gig5udgbHlJRXQqcXDNZmYssMERGRcUw6zHh7e6NLly4G2zp37oy0tLQGz1EqlXB0dDR4tJaHerdHhL8zhnf2gLWlHF8/1stgf65GiwqdYR8ZtswQEREZx6Q7AA8YMAAJCQkG2xITE+Hv7y9RRTe26KEeBs87ezsYPM9Vl9VpidGxAzAREZFRTLpl5pVXXsGhQ4fwwQcfICkpCT///DO+//57zJgxQ+rSGkUmk+He7t7i81yNts5Q7EqdAE1ZBfRsoSEiImoSkw4zffr0wfr167FmzRqEhYXh/fffx+eff45JkyZJXVqjLXqwByb18wMA5KjLUHFdS8yBC1fQ473t+OrvJCnKIyIiMnsmHWYA4N5778XJkydRVlaGs2fP4plnnpG6pFtiYyXH8M4eAKpaZhKyNQb7v9h1HnoB+GxnohTlERERmT2TDzNtgZejDQDg9CU1Ji09LHE1REREbQvDTCsI9XJAj/aqmx5X36KUREREdGMMM63AwkKGzx7picjOnmjnZNPgcZkFpa1YFRERUdvAMNNKOrjbY+mUCGz+z0B4OirrPSbtSkkrV0VERGT+GGZambOdFQ7MGo4g95rZgYd2cgcAvLw2Dn+dzEJhaYVU5REREZkdhhkJyC1kBreUpvQPAACoyyoxffUxTF99FILAeWeIiIgag2FGImUVNfPNDAv1wLyxNcs2/Jt0BfuT8qQoi4iIyOwwzEjMUl61QOUDvdujo6e9uP3xH2Pw9IpYzgxMRER0EwwzEgnxqAouD0f4AgAcrS2x/ZUh+OGJCPGYnWdzsO5ohiT1ERERmQuTXmiyLVvxZF/sPJsjhplqfQNdDJ4v2XsB94e3g6VcBplM1polEhERmQW2zEjEx8kGT/QPgLWl3GC7ysYSj/bxhY/KGgCQnFeMju9sxaPfH5KiTCIiIpPHMGOCPpzQHQdmD8fkO/zEbYdT8pGjLpOwKiIiItPEMGPC3rqnMx7q3V58Hp9eIF0xREREJophxoTZWimw6KEeeLRPVb+auLQCaQsiIiIyQQwzZqCXvzOAqs7AAbO24D9r4iSuiIiIyHQwzJiBcT19ENnZQ3y+6fgl9p8hIiK6hmHGDCgVcnz/eATeHNVJ3BabehUAkHG1BBU6fUOnEhERtXkMM2bCwkKG6UODMfXOAADAkdR8LN5zAQMX7sY7609JWxwREZGEGGbMTL9rk+otP5CKhdvOAQA2Hr8kZUlERESSYpgxM8NCPaCysTTYprDgzMBERHT7YpgxM9aWckzqVzWZnqN11WoUGm0lknI1GPnZPtz71T84l62WskQiIqJWxTBjhl69uyO2zRyEw29Fwtm2qpXmh30pSMjR4FSmGuvjMiWukIiIqPUwzJghhdwCoV6OsLGSw1tlAwD4JTZd3J9TyGHbRER0+2CYMXNXS8rrbMtRayWohIiISBoMM2Zu2oCAOts4oR4REd1OGGbM3FMDO+CRCF+DbQwzRER0O2GYMXNyCxn6B7kabCsu1+FY2lUs3nMBU5fFoFhbKVF1RERELU8hdQFkvHbONnW2PfDtAfHnf87nYVSYV2uWRERE1GrYMtMGBLnbiz93a6eqsz8tv7g1yyEiImpVbJlpA1zsrLD5PwNhbWkBQQB+3J+CX2LTIQhV+y9eKZG2QCIiohbEMNNGhNVqkflwQne8endH/H0uF7P+OMkwQ0REbRpvM7VRHo7W6HDt9tPZLDXOZavx65F0CNXNNURERG2EyYeZefPmQSaTGTxCQ0OlLsssBLjZAgCuFJdj1Of/4M3fTyD6dLbEVRERETUvkw8zANC1a1dkZWWJj/3790tdklnwcLAWF6WsFpdWIE0xRERELcQswoxCoYCXl5f4cHNzk7okszF/XJjB84yCUgCAXi9gTUwaEnM0UpRFRETUbMwizJw/fx4+Pj7o0KEDJk2ahLS0tAaP1Wq1UKvVBo/bmdxChmGd3MXnZ7Oqrsf6uEzM/uMkRny2T6rSiIiImoXJh5l+/fph+fLl2LZtGxYvXoyUlBQMGjQIGk39LQpRUVFQqVTiw9fXt97jbiefPxqOF4cFAwBS8opxOPkKDiVfkbgqIiKi5iETzGx4S0FBAfz9/fHpp5/iqaeeqrNfq9VCq61ZNVqtVsPX1xeFhYVwdHRszVJNzmu/HsfvxzLQy88J/q52WB+XCQBI/XCMxJUREREZUqvVUKlUjfr+NvmWmes5OTmhY8eOSEpKqne/UqmEo6OjwYOqzIwMAQCcyChEUa31mkrKuXYTERGZL7MLM0VFRbhw4QK8vb2lLsXstHe2gZu9EpV6weA205WicgmrIiIiMo7Jh5nXX38de/fuRWpqKg4cOIDx48dDLpdj4sSJUpdmdmQyGXr5OQEANGU1rTH5xQwzRERkvkw+zGRkZGDixIno1KkTHn74Ybi6uuLQoUNwd3e/+clUx7ie7epsY5ghIiJzZvJrM61du1bqEtqUMd29UVrRA6//dlzcdoVhhoiIzJjJt8xQ8xsf3g5je/iIz/OLtTc4moiIyLQxzNyG5BYyfDUxHE8PDAQApOWXILOgFPM3nUF6PlfYJiIi88IwcxvrH+QKANh2KgfTlsXg//5NwWu1bj8RERGZA5PvM0MtZ3BHdzjbWiKvSIu8oqpbTTEp+RJXRUREdGvYMnMbs5Rb4LkhQQbbPByUElVDRETUNAwzt7lnB3XAoBCuQk5EROaLYeY2Z2Ehw2eP9MQdHVwAAJeLtKjQ6SWuioiIqPEYZghu9kr8/PQdUFjIIAjA8n9TkZRbJHVZREREjcIwQwCqWmg8Ha0BAAv+OovIT/carN9ERERkqhhmSNTOycbg+YzVx7jUARERmTyGGRKFX1uEstqV4nLsSciVphgiIqJGYpghUd9AF/FnH1XVLaf0/FKpyiEiImoUhhkSRfjXhJmRYV4AgB/+SWZnYCIiMmmcAZhEKltL/PRkX+j0AtRlFQCAIm0lHlxyAPFzR0hcHRERUf3YMkMGBnd0x7BQD/i62IrbCkoq8PLaOBRrKyWsjIiIqH4MM1Qvv1phBgD+jL+ExXsuSFQNERFRwxhmqF6udlYYGGy4zEHG1RKJqiEiImoYwwzVSyaTYdXT/aBU1PyK5BVxzhkiIjI9DDN0QwNqtc4k5GgAVHUKnvJ/MVj6T7JUZREREYkYZuiGPnqwOx6OaA8AuKzRIr+4HD/sS8bexMv435azEldHRETEMEM34WavxEcP9oCvS9VSB6cyC3Eio0DczxFOREQkNYYZapROng4AgCf+Lwa7Ey6L2y9eYadgIiKSFsMMNUrHa2HmehevFLdyJURERIYYZqhRfK5bUbtaKltmiIhIYgwz1Cgju3rBzV5ZZztbZoiISGoMM9Qo7g5KxL4TiXbXWmj6d3AFAKQyzBARkcQYZuiW/Pp8f3w1MRxvjOoEADiUnI9fY9Px6PcHEZOSL3F1RER0O2KYoVvSzskGY3v4IMDVTtz25roTOJScj6eWH5GwMiIiul0xzFCTONta1tmm0VZCrxdwNkuNSp1egqqIiOh2xDBDTSKTyerd/uSKIxj9xT/4ZEdiK1dERES3K4YZarKPHuwOdwfDEU57rk2ot3jPBSlKIiKi25BZhZkPP/wQMpkMM2fOlLoUAvBwhC+OvB2JmLeGS10KERHdxswmzBw5cgTfffcdunfvLnUpdJ3rW2cAwKmePjVEREQtwSzCTFFRESZNmoQffvgBzs7OUpdD16mv/4y2gh2AiYiodZhFmJkxYwbGjBmDyMjImx6r1WqhVqsNHtR6HK0VAIDSCh1X1CYiolZh8mFm7dq1OHbsGKKiohp1fFRUFFQqlfjw9fVt4QoJAJ4cEAgAWDatD2ws5QCAU5mFeHL5EYz8bB/SGrmGU3x6AV7/7ThyNWUtVisREbUtMkEQBKmLaEh6ejoiIiKwY8cOsa/M0KFD0bNnT3z++ef1nqPVaqHVasXnarUavr6+KCwshKOjY2uUfVsSBAEFJRVwtrPCoI/+Rnp+KeyVChRda515c1QnTB8afNPXCZi1BQBwTzcvfDupd4vWTEREpkutVkOlUjXq+1vRSjU1ydGjR5Gbm4tevXqJ23Q6Hfbt24evv/4aWq0Wcrnc4BylUgmlsm6HVGpZMpkMznZWAAA3eyXS80vFIAMACdmaW3q9Wz2eiIhuXyZ9m2n48OE4efIk4uPjxUdERAQmTZqE+Pj4OkGGTEOYj6rm53ZVabo6nMz4+RhGf/EPCksqbvgalnKT/tUkIiITYtItMw4ODggLCzPYZmdnB1dX1zrbyXT8d3Qoyiv10Ggr8OrdnRD56V5cuFyE4+kF2HIiCwDw4/5kvDqiE9LzS/DuxtN4ckAgBgS7iq/BMENERI1l0mGGzJO9UoGFD1b1cRIEAQ7WCmjKKjH7j5PiMWuPpOPVEZ0w9uv9KCipwLksNaJfGSzut5RXDfdOu1KCrMJS9OvgCiIiovqYXZjZs2eP1CXQLZDJZLgr1AN/xl/CmayaYfK5Gi1OZRai4NrtJnVZJS5rajpuayur5qkZvGg3AGDXa0MQ5G7fipUTEZG5YFs+tbjRYd61fvZCOycbAMDqwxfF7S52VsitFWYKSiqQVVgqPk9kh2AiImqA2bXMkPmJ7OyBpwYGwl6pwPNDgvDC6qPILCjFmph08ZgrRVqDlpnMglLc/82/4vPqkVGFJRWwVMhga8VfXSIiqsJvBGpxCrkF5tzbRXwe4mEvrq5drbhcV2c4do66JtxcLtKisKQCwz/dA2+VDTb9Z2DLFk1ERGaDYYZaXahX/ZMffb07qcFzLmu02J+Uh7yicuQVleNqcbk4rw0REd3e2GeGWl1v/5rFQj0dlbBXNpypXa4FlssaLRJzalpukvOKWq5AIiIyKwwz1Or8XW3Fn60UFgYzBQNAsEfNqKXu7asm4Lus0eJERoG4fcuJbGw6fgm1V+O4VFCKb3YnoaCkvIUqJyIiU8QwQ62uerg2ADw/JMhg35ujOqGrT81tqOrZhJPzinEoOV/c/n//puA/a+Lw5roT4mzC05YdwaLoBCzYcrbe9xUEASa8FBkRETUR+8yQJD57pCeOpxdgUIgbNGWVWBSdgNVP98MdHVzxxc7z4nE9fZ0AwGCkU22/Hc1ASYUO7ZxskHDtNtT2MzlYdN1xFTo9Hlx8ALZWCvz8TD/IZLKW+FhERCQBhhmShMrGEoM7ugOoap2ZemcArC2r1trydrIWj+vbwQWvRHbEttPZyFGXYd59XfHSmjiD14o+lY1KfU2Li6NN3V/rExkFOJ5RCADQaCvhaG3Z7J+JiIikwTBDJqE6yABAkLud+LODUoGXI0PwcmSIuM3F1go//JOMZwd3wKSlhw2CDACUVehxJDUf/5zPQ1zaVXz/eAROX6qZffhKUTnDDBFRG8IwQyanl58z3hzVCX4utvXeDhoY4oaBIW7Q6QVYymWo0BmGmcsaLR5aclB8vjfxMuLSCsTnsan52J+Uh2X/puCRCF88d12/HSIiMi8MM2RyZDIZpg8NvulxcgsZfF1skXy5GADQN9AFRy9ehe66lprTlwqx80yO+PyNdSfEn6O2nmOYISIycxzNRGYtwLXmltTwUA+EtVPVOearv5OguW74NxERtR0MM2TWwq+NdgIAHycb9GxfE2ZqD/EGADsrOerD4dpEROaNYYbM2pMDA+GgVEAmqwov3ds7ifsGBLsZHDv02tw21yut0LVkiURE1MIYZsis2SkV2P3GUGycMRAd3O0NbjP17+Bac5yVHH1qLaNQ29WSChxLu4px3/yL2NT8eo8hIiLTxTBDZs/NXolu124vdfJywFv3hGLhhG4IdKvpT9PTzwmejjXz17R3toGnoxIA8PPhi3jg2wM4nl6AB5ccNLjt9M/5ywifvx07anUgNtaR1Hyk55c02+sREd3uGGaozXl2cBAe6eOH9s426OHrhO7tVXjvvjAE1VrzycXOCs62VYtYfrP7gsH5JzIKkVekRUxKPh7/MQZXSyrw3MrYBt/vXLYaD3z7Lw4k5dW7v3Y4SrtSgoeWHMSTy48Y8xGJiKgWDs2mNksht8CG6XeKc9VU6PTivpJyHVyuhZnr7U/Kw2+x6Ui9UtN6oheApf8kI9zP2WDVbwCYs+EUjqUV4LGlh5H8wT2wsKiZG0evFzD5x8O4rNFi44sDkXS5asmF87lFuFpcDme7+mtoDYWlFbCzkkMh579piMi88a8YtWm1J92zrPWlnasuQ1oDt3oWRScYBJlq/9tyFhMWH4BOL2Dd0QycuaRGpU5vMCHfgQtXDM7ZfiYHBy5cwfncIsSlX0V2Yc0aU2ey1LiRSwWldebMaS6nMgvR53878e7G0406XhAEgzBIRGRK2DJDtxUruQXKdXr4udqiqKxpc8+88ks8Nh6/BJWNJR7t62uwnMKOM9nI1ZRheGdPqGws8dPBVHHfv0l5sKgVruLTC+BqbwVNWSV2ns1BH38XdGuvgqejNXYn5GLasiPo4euElU/1venyC3/GZ6JCJ6CorAIPRfjCTnnj/7XfXn8S5To9Vh9Ow4Lx3bAnIRddfVRwd1DWOVanF3DvV/uhrdDhj+l3wqlWi1ZhaQUgAPbWCmgrdbC14p8UImp9MqGNT7KhVquhUqlQWFgIR0fHm59AbdrJjEJ8sSsRb44KRZG2Eu9tOoOx3b3xvy1nEdbOEacyb9xa0pCOnvZIzCkSn4/p7o3/jQtDxIKdt9S6Euhmh12vDsHcjaew6lAaAMBHZY0gD3uEeDhg5t0hcLS2RHmlHjnqMvg42eBkZiHu/+Zf8TXaO9vA2dYKj/XzQ7ifE6wVcgS42WH76WzYWMkxMNgNYe9Go7i8akj6d4/3xnMrj8LdQYkDs+4yaMECgJS8Ygz7eA8A4JEIXyx8sDsAoFKnx9CP96BSJ+Cuzh5YF5uBP18cgM7eDf9/Fpuaj6slFbi7i6e4raCkHLGpVzG8s0ed5SsWRZ9DYk4RvnmsF6wUt29Dcmm5Dmez1ejZ3sngNibVVVahwy9H0nFXqAd8XWzrPUavF7DpxCXc0cHVYGAAmZZb+f5mmCECkJijgaejNU5kFECnF6DTC3hqRVWn3xFdPLH9BqOZevs74+vHwtE/6m+D7X0CnHEk9eot1zIoxA3/nK+/M/HIrp44lalGZkEpAKCTpwM8VdbYl3i53uOtLS2gsLDAD09EYOIPhwAAiyf1wgurj4nH9A10QUxK1ZD0VyI7wkIGDOrojp7XJiSMPp2N51YeBVA1cmzTfwbAW2WDY2lX8cC3Bwzer6evE+4K9UAPXydYyKo6U0/o1R5eKmvo9AKC3voLAPDPm8PEL5qRn+1DQo4Gnz/SE/eHtxNfS1NWgW7ztgMAFowPQ0FJBZ4aGGiwKGlT5BeX47JGi05eDk06XxCEetcMq+18jgbaSj3C2qlQXqmHXhBgbSlHdmEZrpaUI9TL4YavUanTQ24hE4+ZsfoYtpzMwsIJ3fBIH78m1d1YZRU6/HM+D0M7udcJtk11LluN+ZvO4NW7OyIiwKVZXrMhn2xPwFd/J6Gdkw3+nXVXvcesO5qB1387DncHJY68HQmgqk9dYz5vY/77N1WxthLpV0sQ6lX/d1XG1RIs2HIWTw0MbPHr2BBBELDzbC4UFjIMCnFr0T53t/L9zTZhIgAdPau+2AaFuAOo+jIJ9rBHpU6Pif38xDDzzWO9YCmX4dlrX+4AsO75/pDJZHhucAf8fDhNXDqhOsjMG9sF53OLsPpwWp339XK0xqcP90B3Xye89ms8ok/nNBhkACD6tGGoSsjRICFH0+DxZRV6AHoxyAAwCDIAxCADAJ/tTAQA/N+/KfB0tMa5bMPXzivSon/U3/ji0Z719vmJTy9AfHqBwbb95/Pww5QIrD50Udz2a2w6XhgahFWHLor1f7IjAa72Vugb6AKlQo7YizVB8O31pwBUtRJN6ueHzt6O9YYaTVkF1GWVaOdk0+A1mbz0MM5kqbHlpYHo6lN3+Ysb+WjbOaw9ko4hHd3xYO/2GBDshsKSCiw/kIrH+vnB3UEJbaUOd3+2DwBw9J1ITFkWg4KSCkTPHIwJiw8gs6AUk/r54b+jQzF56WHc0cEVM4YGQ6vTwcPBGgUl5Rjx2T70CXDBN5N6AQC2nMwCAHy5KwnOtla4I8gVjtaWSL5chN+PZWD60GCDW4u/xqbj3T9PY1ioOz59uGeda5VdWIaX1sZhQJAbXrwrGMczChDkZg+VrSXe23Qaa2LSYaWwwLyxXfFYv/rDU+0v9c93JmJDXCZ+fa4/9AJwNlsNa4Uc/q628HGywQurjiElrxgnM47g5Hsjb3iNv9t7AVmFZZh7bxeDVqjC0gqUlFfCW2WDvYmXcamgFBP7VtV2PL0AOkFAuK+TeK2qA3999l4L/5c1WmjKKrDpeBbe2XASiyf3xsiuXuJxuxNy0dXHET/+kwIbKzmeHxKEB5ccgIPSEj8/06/BUJN2pQR6QUBArekhGuPdjaex7mgGPn6oBx7s3b7O/vmbzmD7mRxsPZWN1A/H3NJrX69Sp683iFwqKIWrvRWUivr/0bA38TKe+anqH3pz7u2CpwYGGlVHc2GYIaqHQm6Bzf8ZCEEAymt1fO0T6AwPB2s8NTAQP+5PwaAQN/EP2ux7OmP2PZ2hKavAd3uTcTyjACO6eGLyHf6QyWS4t7uPQagAAFulHHdem6n43u4+dcJKtR7tVTieUWiwrauPI05fqhsoJvXzqzc41eZiZ4XBIW7YEH+p3v1XSypwtaSiwfNfXht/w9ev7WDyFYS9G22w7au/k/DV30kG29LzS/H4jzF4ckAg3ronFJ/vSKzzWuuOZmDd0QwM7eSOZVP7YPuZHHy7Ownzx4Whk5cDJiw+gNQrJdg+czDkFjJ8vy8ZHb0c4GZnhbj0Ajw3uIMYwn6LzcC5dhpYKiygVFgg3NcJHo7WSMkrxq6zVR23H+vrh5WHLqK3vzOmDw3Ct3uqhvGvj8vE+rhMHH5rOGb/cRJ/n8vFZzsTMbSTuxiIgaoO4NW3Lv+MvyR+wa4+nAaVjSVOZBTiREbVQqiXi7TY9doQRJ/OQa5Giy0ns/BWQalBMMssKMWzK4+iT4Azfnv+Tjz83UHkFZXjSlE5xvVsh67tHOFobYmtJ7NQWqHDXyezMaRjZp3WnDUxaYhJyUdMSj7OZBUi+nQOBga7YdXT/bAmJh0AUF6px1vrTyKyswc8HK2Roy6DXhBQXqnHfV//i/t7+uC9cWHILy7H5zvPi59xy8ksMdB28nRA9CuDkZJXtRjs9WukHU6+Ah8nG7GVLr+4HFFbzwEA7uvpg15+ztBW6vDDvmR8vD0RNpZy7HxtCKb8XwwAoEd7JzhYKzBh8QFU6gXMjAyBtqLm/1d1WQWs5BawtpSjrEIHuYWsTuvLv0lX8Nb6kwCA51Yexfv3h+HxO/wNWiSrKSxk4n/PL3clYcqd/gZ9yP5NykOOugxvrDsBOys5Ds4eDrmFDO9tOoNBIW64p5v3de+dB39XW7R3thV/vwHg9d+OI9jDXmwdBYDtp7MbbCEur9Tj92MZ6OTlgC7ejvj9WAbu6OCKrIIyfBR9Dk8NDMSh5Hy0c7LGi3eF4FRmIR5ccgAT+/phzpgu2Hj8EvoGuiAlrxiTfzyMx/r6YcH4bvW+15FaE4vGpuYzzBCZuup/zdpAjq8mhqO8Ug8Ph6r762+M7IQQD3vc0927znkO1pZ4fWSnOtv7B7lizr1dkKspw+bjWcgsKMXY7j7i/juDamYsdra1xPdPROChJQcBAMNCPaDRViL5cjFmRobgskaLV+7uiC93ncdPBy/C39UWF6+UwMZSjgXju8HGUo6l+1MAAEM6uuOf85dRu+vOhw90g8rGUgwzVnILrHyqLw6n5MNKYYEPr32hSOG3o+ko0lbUCW+17Um4jD4LdkFTVgFtpR7jvvkXTw0MFPstDb3Wx+d6J2u95vIDqQb7VDaWeO++rpj5S7y47e9zuQCq/jW6IT6zzut9tiNRPKa6rj0JNbf8qr+cAGB9XIbBuUv/SRF/Tr72ZT9zbbzBiLgBHxreuqx2JPUq0q6UIK+oHACw9kg61h5JF/s0pV+taZX4bm8ysgu1OJNViJeGV/W5+mLXeXF/dYDen5QHbWXdpT32J+Who6cD7vt6P/RC1e9mYWkFVhy8CAdrS3y9uyaUxl7MN2iZS8jRVHUSv86BpDy8v+Uszmap4ediiz9nDMDCbedwqbBMPOarXecht5AhrJ1KDEulFTos2VMzL9SFy0Wo0OnFTvgxKfnQVtaEmQFRf8PfzRYPhLfHh1vPoVynh71SgaJaoeroxXxYymWo0FW9xpwNp3B3Z896J8r8eHtNwP5sZyJWHkrFksm98dnORAzr5IH/bTkr7leXVeLX2HR8vy8ZWYVlWBOThnPvj0KFTo8N8Zfg62yDqcuOwNZKjmAPe/FvS7WX18Zh7xvDkHy5CJ/sSMSWE1kG+wtLKlCh1+PAhStIyFbjm90XIJMB9XUcqf2Pj46eDog+nYOyCj2W/ZuKkxmFiL14FXcGuSIxRwNBqArbjjaWyCooRXG5DiEe9rgr1AN5ReUG83KdvcmIzNbEPjNEEkjPL8GehFxM7Otn0NQbMGsLgKp+M0sm90bXay0aC8aHYVgnD1wpKhdnOwaqmoqTLhch2N0eyw+kYlioB4Lc7VFSXond5y7Dxc4K/YNcoa3UYc6GU/g1tuoL9cCsu+Ctskbkp3tx4XIxPhjfzeB2wqbjl/CfNXHi86VPRODpn2omDpTJgFAvR0y7MwBv/n7CqGsR4Gpb71B4oG7HaoWFDMM7ezTYgnU76eXnhGO1pgW42XapeDlaI1tdE1Leu69ro6cEuBkHawU0tUYlXv9eTfXy8BCkXy3BH8fqBtjWNLaHD7adyhKDVm0/TonAB3+dxYXLxRJUVuOl4SEYHeZ1w47/TXUr39+37/AAIgn5utji8f4Bde5Zf/JQDwS42uLdsV0M+kAEudvDx8nGIMgAVbfDQr0coZBb4OlBHRDkXjXLsa2VAmO6e6P/tdYepUKO8lr/YvVWWUMmk+H/pvap6hzc19fgdcf28MGe14fCzkqOiX39ENnFE++M6QwAmDEsCNEzB2Pts3egb6ALFBYyhLVzxEO17vFbW1pgxyuDxefjevpg1ujQeq/FrNGdG7xO3zzWS/x5UIgbdrw6BN89HoHYdyLRL7CqA2QHdzt4q6r+VSu/yUgfq1vorPjGyE54sHd7g2UxaqtvGHtjLHqwO5xsbzzUvjEaCiy1t//+Qv96j7G1kuM/dwUbXUO1IR3dG9x3fbhoapCpXn6kNs110ysYE2RkMoi/o1/sOt/kIPPVxPB6+7s0VkdPewRfm6180/FL9QYZAHhqRWyjgkxLjAK0trSAyqbqd/jLXecxdVkMyiResJe3mYhMyITe7TGh1h/C5dP6ICm3SPziNsadwVV9ZGyt5GI/H39XO/i71v9lHeBmh+PvjhDnxpk2IBB9A13Q1UclhgaVjSX2//cuONtZIuNqKTYev4RBIe5YOiUCQNWw799i0zH33i5wtVdiYh8/3PPlP+gX6IL+Qa7IUZcZDNPu5Okgdgh+eXgIQjwdcGDWXThw4QqGdHQXA4SbvRIrnuyLLSeyMCDYDdpKHT7enogJvdqhvbMNvt+XjF9jM/DRg92RmK1BWaUOLw0PgbOtFYq1lVi85wK+25ds8Hk3zBgAe6Ucfi52kMlqJllceTAVc/40/AKeemcAXonsiB1nc7D0n2Scy9bASm6BRQ91x4BgNxRrKzFk0Z4619TBWoFxPdvB1d4KvxxJR0K2BqlXStCjvQod3O2xPu7GX6A2lnJ0a6fC4I5u2Hc+D34utghwtUWRVocley/UOb63vwsev8MfKw9dxAO92mHryWyEeNrj64m94OGorNNvCQCcbC2x6cWB8HS0xt7Ey3huZSyun13AzV6Jnr5O+Of8ZayfPgAO1goM+mi3wTFLn4jAC6uPwsHaEm/d0xm/H83AweSqW2jDQz1gbSVHfFoBynV6TOrnh6l3BmDrqWyE+znhnfWnDDqAA8DdXTzF6QrG9vDBpuP19/dqjABXW/QJcMFvtW4DPjkgEA9H+GLziUsGUzS42SshtwBy1Nr6XqqOEV09EeRub3CLsVq3diqcz9Vc65hfZekTETiRUYAvr/236NHeCYWlFUjKrWqRfOueUDzRPwDPrTwqdlyuz7ODO+Bw8hUk5xWLIW9sDx/4qKzF3/V5Y7sg42qpeAu6moO1AsXaStwf3g5Xi8uxO8HwfWq3zA7u6I6Xh4fg2MWrWPBX1W21HLUWqw5dxNODOjTqGrUE3mYiuk3o9AJ+P5aBfoEuDQYYY13WaGGvVMDG6taGT3+x8zxS8oqw8MHuyC8uh7eq4dFIjaHXC8i+Ng9PfSp0evxyJB092jth2vIYWFvKsef1ofWO7tiTkIupy2rW0ors7IGlU/oYHFOp06OkQmcwuWF1CHp+SBDaOdvgo23n8M6YzgadcbMKSzFvY9UxHT0dsOrQRQwL9UBijgYv/hyHN0Z2wrZT2TiZWdXX5/R7I6FUWNRbZ/UtytpSPxyDSp0eCTkadPF2hE4vGJzbZ8FOXNZoEfVAN2w7lY3Y1HxsfmmQQWuUIAhYFJ0AG0s5TmYW4u9zuVj1dD/0CXBBWYUOdkoFdHoBIz7bK7YUfPRgdzwc4YvySj0UFjJYWMiQmleM//s3BY7Wlnj17o43nC+nvFKP2Iv5eOyHw+K2byf1wvRrI/F2vjoE9329H+2dbbDqqX7QaCsx/JO94rH9Al1wOCUfI7p44rF+ftBW6uFmr8Qj3x3Ec0M64I2RVS0wQxftRuqVEnz3eG+M6OIJmUyG8ko97vpkDzKulsJCBpx6byRsrRQoKCnH+dwirIvNwC+xVZ2kD781HH/GZ2L76Rwk5Gjw45Q+6HvtHx7Jl4uQV1SOXn5OsJDJkJxXjCB3O2i0lbCxlOOPYxko1wl4/A5/AFWdu9ccTsNDEe0Rn16Al9fG4/6ePvjskZ6QyWRYvOcCFm6r6sv22t0dcSZLjUf7+omdoVc/3U/sMJ1xtRSrD6fhv6M6obhcJ/a9+uXZOxDu54xXf42Hn4strC2rRg1+/Vg47KwUkFvIkJJXjDUxaRjRxROv/3Yck+/wx9ODOuBkRiE02gr0C3QV/zFTWFKB346m46PoBLw4LBgvDQ9p8L9pU3CemVoYZojoRvKKtJDLZA2uk3XhcpH4RTmupw/eHdsVLo1cUys9vwTtnW0gk8lueX6SwpIKOFgrkJxXjFd+iceMYcEYFebV4PG1w0wvPydMGxCIsT18GjweAM5cUuN8rgb39fCBTi+gQifcMIiWlFciv7hcHH1TW1mFDnpBgCDAoPXPGCsPXcScDafQrZ0Kvz3fH0+tOIKuPiq8dU9n5GrKYGMph8O1AFn9+eUWMux5fSi2nMzClP4BBp+nrEIHK7mFGKTS80uQWVCKOzq4GrzvpYJSLPs3BcEe9nVGgq04kCreKqs9PLqhoc5NIQgCUq+UIMDVVryOtUdX1X7fbaeykXG1BE8NDGzwmv9yJA2JOUV4+57OzT7pYoVOjytF5fBSNf/kgwwztTDMEJExtJU6dJkbDUEQcPb9UQ3OvyG13u/vwJXicozq6oUlj/eWupxmcz5HAxc7K7ja37iPUnWYqR4O3lLKK/X4+u/zGNLJo86isy1JrxewMPocurVT4d7uNw6pbUWbCjOLFy/G4sWLkZqaCgDo2rUr5s6di9GjRzfqfIYZIjJWrroMkKHO8FlTcjZLjbUxaXhpeMhNv/jboj+OZeDTHYn47vHetzwZIpmmNhVmNm3aBLlcjpCQEAiCgBUrVmDRokWIi4tD165db3o+wwwREZH5aVNhpj4uLi5YtGgRnnrqqZseyzBDRERkftrs2kw6nQ6//fYbiouL0b9//fMnaLVaaLU1Q+jUatOZoZCIiIian1lMmnfy5EnY29tDqVTi+eefx/r169GlS5d6j42KioJKpRIfvr6+9R5HREREbYNZ3GYqLy9HWloaCgsLsW7dOixduhR79+6tN9DU1zLj6+vL20xERERmpM33mYmMjERQUBC+++67mx7LPjNERETmp82vzaTX6w1aX4iIiOj2ZfIdgGfPno3Ro0fDz88PGo0GP//8M/bs2YPo6GipSyMiIiITYPJhJjc3F0888QSysrKgUqnQvXt3REdH4+6775a6NCIiIjIBJh9mfvzxR6lLICIiIhNmln1miIiIiKoxzBAREZFZY5ghIiIis8YwQ0RERGaNYYaIiIjMmsmPZjJW9QTHXHCSiIjIfFR/bzdmoYI2H2Y0Gg0AcMFJIiIiM6TRaKBSqW54jFmuzXQr9Ho9Ll26BAcHB8hksmZ97epFLNPT07nu003wWjUer1Xj8VrdGl6vxuO1aryWulaCIECj0cDHxwcWFjfuFdPmW2YsLCzQvn37Fn0PR0dH/rI3Eq9V4/FaNR6v1a3h9Wo8XqvGa4lrdbMWmWrsAExERERmjWGGiIiIzBrDjBGUSiXeffddKJVKqUsxebxWjcdr1Xi8VreG16vxeK0azxSuVZvvAExERERtG1tmiIiIyKwxzBAREZFZY5ghIiIis8YwQ0RERGaNYaaJvvnmGwQEBMDa2hr9+vVDTEyM1CW1un379mHs2LHw8fGBTCbDhg0bDPYLgoC5c+fC29sbNjY2iIyMxPnz5w2Oyc/Px6RJk+Do6AgnJyc89dRTKCoqasVP0TqioqLQp08fODg4wMPDA/fffz8SEhIMjikrK8OMGTPg6uoKe3t7TJgwATk5OQbHpKWlYcyYMbC1tYWHhwfeeOMNVFZWtuZHaXGLFy9G9+7dxQm4+vfvj61bt4r7eZ0a9uGHH0Imk2HmzJniNl6vGvPmzYNMJjN4hIaGivt5rQxlZmZi8uTJcHV1hY2NDbp164bY2Fhxv0n9jRfolq1du1awsrIS/u///k84ffq08MwzzwhOTk5CTk6O1KW1qr/++kt4++23hT/++EMAIKxfv95g/4cffiioVCphw4YNwvHjx4X77rtPCAwMFEpLS8VjRo0aJfTo0UM4dOiQ8M8//wjBwcHCxIkTW/mTtLyRI0cKy5YtE06dOiXEx8cL99xzj+Dn5ycUFRWJxzz//POCr6+vsGvXLiE2Nla44447hDvvvFPcX1lZKYSFhQmRkZFCXFyc8Ndffwlubm7C7NmzpfhILWbjxo3Cli1bhMTERCEhIUF46623BEtLS+HUqVOCIPA6NSQmJkYICAgQunfvLrz88svidl6vGu+++67QtWtXISsrS3xcvnxZ3M9rVSM/P1/w9/cXpk6dKhw+fFhITk4WoqOjhaSkJPEYU/obzzDTBH379hVmzJghPtfpdIKPj48QFRUlYVXSuj7M6PV6wcvLS1i0aJG4raCgQFAqlcKaNWsEQRCEM2fOCACEI0eOiMds3bpVkMlkQmZmZqvVLoXc3FwBgLB3715BEKqujaWlpfDbb7+Jx5w9e1YAIBw8eFAQhKrwaGFhIWRnZ4vHLF68WHB0dBS0Wm3rfoBW5uzsLCxdupTXqQEajUYICQkRduzYIQwZMkQMM7xeht59912hR48e9e7jtTL03//+Vxg4cGCD+03tbzxvM92i8vJyHD16FJGRkeI2CwsLREZG4uDBgxJWZlpSUlKQnZ1tcJ1UKhX69esnXqeDBw/CyckJERER4jGRkZGwsLDA4cOHW73m1lRYWAgAcHFxAQAcPXoUFRUVBtcrNDQUfn5+BterW7du8PT0FI8ZOXIk1Go1Tp8+3YrVtx6dToe1a9eiuLgY/fv353VqwIwZMzBmzBiD6wLw96o+58+fh4+PDzp06IBJkyYhLS0NAK/V9TZu3IiIiAg89NBD8PDwQHh4OH744Qdxv6n9jWeYuUV5eXnQ6XQGv8wA4OnpiezsbImqMj3V1+JG1yk7OxseHh4G+xUKBVxcXNr0tdTr9Zg5cyYGDBiAsLAwAFXXwsrKCk5OTgbHXn+96rue1fvakpMnT8Le3h5KpRLPP/881q9fjy5duvA61WPt2rU4duwYoqKi6uzj9TLUr18/LF++HNu2bcPixYuRkpKCQYMGQaPR8FpdJzk5GYsXL0ZISAiio6Pxwgsv4KWXXsKKFSsAmN7f+Da/ajaRqZkxYwZOnTqF/fv3S12KyerUqRPi4+NRWFiIdevWYcqUKdi7d6/UZZmc9PR0vPzyy9ixYwesra2lLsfkjR49Wvy5e/fu6NevH/z9/fHrr7/CxsZGwspMj16vR0REBD744AMAQHh4OE6dOoUlS5ZgypQpEldXF1tmbpGbmxvkcnmdHu45OTnw8vKSqCrTU30tbnSdvLy8kJuba7C/srIS+fn5bfZavvjii9i8eTN2796N9u3bi9u9vLxQXl6OgoICg+Ovv171Xc/qfW2JlZUVgoOD0bt3b0RFRaFHjx744osveJ2uc/ToUeTm5qJXr15QKBRQKBTYu3cvvvzySygUCnh6evJ63YCTkxM6duyIpKQk/m5dx9vbG126dDHY1rlzZ/G2nKn9jWeYuUVWVlbo3bs3du3aJW7T6/XYtWsX+vfvL2FlpiUwMBBeXl4G10mtVuPw4cPiderfvz8KCgpw9OhR8Zi///4ber0e/fr1a/WaW5IgCHjxxRexfv16/P333wgMDDTY37t3b1haWhpcr4SEBKSlpRlcr5MnTxr8cdixYwccHR3r/NFpa/R6PbRaLa/TdYYPH46TJ08iPj5efERERGDSpEniz7xeDSsqKsKFCxfg7e3N363rDBgwoM70EYmJifD39wdggn/jm7U78W1i7dq1glKpFJYvXy6cOXNGePbZZwUnJyeDHu63A41GI8TFxQlxcXECAOHTTz8V4uLihIsXLwqCUDVsz8nJSfjzzz+FEydOCOPGjat32F54eLhw+PBhYf/+/UJISEibHJr9wgsvCCqVStizZ4/BsNCSkhLxmOeff17w8/MT/v77byE2Nlbo37+/0L9/f3F/9bDQESNGCPHx8cK2bdsEd3f3NjcsdNasWcLevXuFlJQU4cSJE8KsWbMEmUwmbN++XRAEXqebqT2aSRB4vWp77bXXhD179ggpKSnCv//+K0RGRgpubm5Cbm6uIAi8VrXFxMQICoVCWLBggXD+/Hlh9erVgq2trbBq1SrxGFP6G88w00RfffWV4OfnJ1hZWQl9+/YVDh06JHVJrW737t0CgDqPKVOmCIJQNXRvzpw5gqenp6BUKoXhw4cLCQkJBq9x5coVYeLEiYK9vb3g6OgoTJs2TdBoNBJ8mpZV33UCICxbtkw8prS0VJg+fbrg7Ows2NraCuPHjxeysrIMXic1NVUYPXq0YGNjI7i5uQmvvfaaUFFR0cqfpmU9+eSTgr+/v2BlZSW4u7sLw4cPF4OMIPA63cz1YYbXq8YjjzwieHt7C1ZWVkK7du2ERx55xGDeFF4rQ5s2bRLCwsIEpVIphIaGCt9//73BflP6Gy8TBEFo3rYeIiIiotbDPjNERERk1hhmiIiIyKwxzBAREZFZY5ghIiIis8YwQ0RERGaNYYaIiIjMGsMMERERmTWGGSK6bSxfvrzOqshEZP4YZoio1U2dOhUymUx8uLq6YtSoUThx4kSjX2PevHno2bNnyxVJRGaDYYaIJDFq1ChkZWUhKysLu3btgkKhwL333it1WURkhhhmiEgSSqUSXl5e8PLyQs+ePTFr1iykp6fj8uXLAID//ve/6NixI2xtbdGhQwfMmTMHFRUVAKpuF7333ns4fvy42LqzfPlyAEBBQQGee+45eHp6wtraGmFhYdi8ebPBe0dHR6Nz586wt7cXQxURmS+F1AUQERUVFWHVqlUIDg6Gq6srAMDBwQHLly+Hj48PTp48iWeeeQYODg5488038cgjj+DUqVPYtm0bdu7cCQBQqVTQ6/UYPXo0NBoNVq1ahaCgIJw5cwZyuVx8r5KSEnz88cdYuXIlLCwsMHnyZLz++utYvXq1JJ+diIzHMENEkti8eTPs7e0BAMXFxfD29sbmzZthYVHVYPzOO++IxwYEBOD111/H2rVr8eabb8LGxgb29vZQKBTw8vISj9u+fTtiYmJw9uxZdOzYEQDQoUMHg/etqKjAkiVLEBQUBAB48cUXMX/+/Bb9rETUshhmiEgSw4YNw+LFiwEAV69exbfffovRo0cjJiYG/v7++OWXX/Dll1/iwoULKCoqQmVlJRwdHW/4mvHx8Wjfvr0YZOpja2srBhkA8Pb2Rm5ubvN8KCKSBPvMEJEk7OzsEBwcjODgYPTp0wdLly5FcXExfvjhBxw8eBCTJk3CPffcg82bNyMuLg5vv/02ysvLb/iaNjY2N31fS0tLg+cymQyCIBj1WYhIWmyZISKTIJPJYGFhgdLSUhw4cAD+/v54++23xf0XL140ON7Kygo6nc5gW/fu3ZGRkYHExMQbts4QUdvCMENEktBqtcjOzgZQdZvp66+/RlFREcaOHQu1Wo20tDSsXbsWffr0wZYtW7B+/XqD8wMCApCSkiLeWnJwcMCQIUMwePBgTJgwAZ9++imCg4Nx7tw5yGQyjBo1SoqPSUStgLeZiEgS27Ztg7e3N7y9vdGvXz8cOXIEv/32G4YOHYr77rsPr7zyCl588UX07NkTBw4cwJw5cwzOnzBhAkaNGoVhw4bB3d0da9asAQD8/vvv6NOnDyZOnIguXbrgzTffrNOCQ0Rti0zgzWIiIiIyY2yZISIiIrPGMENERERmjWGGiIiIzBrDDBEREZk1hhkiIiIyawwzREREZNYYZoiIiMisMcwQERGRWWOYISIiIrPGMENERERmjWGGiIiIzBrDDBEREZm1/wfrG9UBmAhyFwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss over epochs\n",
    "losses = [record[\"loss\"] for record in training_history]\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that it shakily decreased throughout the training run. Now let's test out some prompts, and see what our model gives us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tim  The She\n",
      " He\n",
      " And He He The She In He He Today pushed please He It One  As Everyone It He It \" He Suddenly She She You She\n",
      " He His All\n",
      " But\n",
      " He The He They One So He He He He They All\n",
      "\n",
      " She Let The A One One\n",
      " She Tom She\n",
      " It One She It One It He One It One Every The The  It Every\n",
      "\n",
      " He He\n",
      " From They\n",
      " He One\n",
      " She He\n",
      " And The Now\n",
      " His She He One\n",
      " Sheâ took And But He He  The When She It Let She As He It In \n",
      " The\n",
      " They\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_TRAINED.generate(\"Once upon a time, Tim climbed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but could be much worse.. We'll come back to this and make it actually work. I'm pretty sure the model we are using is just a bit less than the smallest TinyStories model (1M), so I assume we can pull this off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
