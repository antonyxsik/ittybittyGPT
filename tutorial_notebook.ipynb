{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IttyBittyGPT \n",
    "\n",
    "Author: Antony Sikorski\n",
    "\n",
    "With lots of help and inspiration from Misha Ivanitsky and his LLM & Interpretability course, and Karpathy's NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we walk through:\n",
    "- Building the model\n",
    "- Setting up the dataset (we use TinyStories)\n",
    "- and training \n",
    "\n",
    "This is far from the most effective available implementation, and there are a number of things that could be improved, but I have found this to be the most effective way to learn how one works.\n",
    "\n",
    "This file has everything you need for cooking up your own little transformer and training it on a chunk of the TinyStories dataset, but doing everything in one Jupyter Notebook is not best practice. The other files in the repo are a more modular and effective way of splitting this notebook up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python_3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# necessary libraries \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from jaxtyping import Int, Float\n",
    "import tqdm\n",
    "import transformers\n",
    "import transformer_lens\n",
    "\n",
    "from muutils.misc import shorten_numerical_to_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if you have a GPU on your computer that you can run this on. That could make this process significantly faster, but you could also run out of memory (Cuda Out Of Memory error). If you don't have torch with CUDA, don't worry about this, you can just use your CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available() == True):\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use my laptop GPU, which is good news! Although I believe it only has 6 GB available, and only 1.5 of them will be available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary auto-reload for development on local machine\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "We first start off with a config class, which allows us to specify the params and size of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class GPTConfig:\n",
    "    \"\"\"Here we configure the dimensions of\n",
    "    our model. We'll set the defaults as the \n",
    "     dims from GPT2 \"\"\"\n",
    "    d_model: int = 768 # dimension of residual stream, the vectors it internally passes around \n",
    "    d_vocab: int = 50257 # defines the number of different tokens that can be represented as inputs (vocabulary size)\n",
    "    n_context: int = 1024 # maximum sequence length (context window size)\n",
    "    n_blocks: int = 12 # number of transformer blocks, frequently called n_layers but I don't like that \n",
    "    n_head: int = 12 # number of attention heads \n",
    "    head_bias: bool = True # whether to use bias in attention heads\n",
    "    mlp_expansion: int = 4 # expansion factor in MLP (they go from small to big to small, this is how many times bigger the middle layer is)\n",
    "\n",
    "    # model dimension must be divisible by number of heads\n",
    "    @property\n",
    "    def d_head(self):\n",
    "        assert self.d_model % self.n_head == 0, f\"'{self.d_model = }' must be divisible by '{self.n_head = }': {self.d_model} % {self.n_head} == {self.d_model % self.n_head}\"\n",
    "        return self.d_model // self.n_head\n",
    "    \n",
    "    @property\n",
    "    def params_shapes(self) -> dict:\n",
    "        return dict(\n",
    "            token_embeddings=(self.d_vocab, self.d_model),\n",
    "            positional_embeddings=(self.n_context, self.d_model),\n",
    "            attention_weights=(\n",
    "                self.n_blocks,\n",
    "                4,\n",
    "                self.d_model,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            attention_bias=(\n",
    "                self.n_blocks,\n",
    "                int(self.head_bias),\n",
    "                self.d_model,\n",
    "            ),\n",
    "            mlp_weights=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                self.d_model,\n",
    "                self.d_model * self.mlp_expansion,\n",
    "            ),\n",
    "            mlp_bias=(\n",
    "                self.n_blocks,\n",
    "                self.mlp_expansion + 1,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            block_layernorms=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                2,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            output_layernorm=(2, self.d_model),\n",
    "            lm_head=(self.d_model, self.d_vocab),\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def params_numel(self) -> dict:\n",
    "        return {\n",
    "            k: int(torch.tensor(v).prod())\n",
    "            for k, v in self.params_shapes.items()\n",
    "        }\n",
    "\n",
    "    # will return the total number of parameters in the model\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return sum([v for v in self.params_numel.values()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separately define the attention mechanism, which is the defining component of the transformer architecture, because it allows the model to preserve long range dependencies by learning what to pay *attention* to. \n",
    "\n",
    "It is essentially some matrix multiplications with a soft-max and a causal mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # store dimensions\n",
    "        self.n_head: int = config.n_head\n",
    "        self.d_model: int = config.d_model\n",
    "        self.n_context: int = config.n_context\n",
    "\n",
    "        # concatenating the outputs of the heads should give us d_model, but this check is done in GPTConfig\n",
    "        self.d_head: int = config.d_head\n",
    "        self.head_bias: bool = config.head_bias\n",
    "\n",
    "        # magic coefficient for scaling the dot product of the query and key in the attention calculation\n",
    "        self.sqrt_dim: float = 1.0 / math.sqrt(self.d_head)\n",
    "    \n",
    "\n",
    "        # key, query, value projections\n",
    "        self.W_K: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_Q: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_V: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        # `register_buffer` means it's not a trainable parameter\n",
    "        # the point here is to not allow the model to \"look into the future\" when making predictions\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", \n",
    "            torch.tril(\n",
    "                torch.ones(config.n_context, config.n_context)\n",
    "            )\n",
    "            .view(1, 1, config.n_context, config.n_context)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_head\"]:\n",
    "        assert x.ndim == 3, str(x.shape)\n",
    "        B, n_ctx, d_model = x.shape # batch size, sequence length, embedding dimensionality (d_model)\n",
    "        assert d_model == self.d_model, str(x.shape)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_Q(x)\n",
    "        k: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_K(x)\n",
    "        v: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_V(x)\n",
    "\n",
    "        # self-attention\n",
    "        att = (q @ k.transpose(-2, -1)) * self.sqrt_dim\n",
    "        \n",
    "        # autoregressive (causal) masking\n",
    "        att = att.masked_fill(\n",
    "            self.causal_mask[:,:n_ctx,:n_ctx] == 0, \n",
    "            float('-inf'),\n",
    "        )\n",
    "\n",
    "        # softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # apply the self-attention to the values\n",
    "        output = att @ v\n",
    "        return output.view(B, n_ctx, self.d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are matrices that need to be learned, and we should check that the dims make sense: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead(\n",
      "  (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "A: AttentionHead = AttentionHead(GPTConfig())\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual demonstration of the causal mask. Although the plot you see is a matrix of $\\{0, 1\\}$ and not $\\{-\\infty, 0\\}$, we use `masked_fill` in the  `.forward()` function to set the elements of `attn` to $-\\infty$ where $M$ is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(A.causal_mask[0, 0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the multi-headed attention component. We compute the attention heads sequentially and then concatenate them, although it is very inefficient. Practical implementations do this all at once with one matrix multiplication, but the indexing for that is difficult to read and harder to learn from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_head: int = config.n_head\n",
    "\t\tself.d_head: int = config.d_model // config.n_head\n",
    "\t\tself.d_model: int = config.d_model\n",
    "\n",
    "\t\t# attention heads from previous class\n",
    "\t\tself.attention_heads: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tAttentionHead(config) \n",
    "\t\t\tfor _ in range(self.n_head)\n",
    "\t\t])\n",
    "\n",
    "\t\t# output projection\n",
    "\t\tself.W_O: nn.Module = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\t# apply all attention heads and concatenate their outputs\n",
    "\t\t# note: in reality, you would do this all in one tensor\n",
    "\t\t# we split the attention heads up to make it easier to understand\n",
    "\t\tatt = torch.cat(\n",
    "\t\t\t[\n",
    "\t\t\t\thead(x) \n",
    "\t\t\t\tfor head in self.attention_heads\n",
    "\t\t\t],\n",
    "\t\t\tdim=-1,\n",
    "\t\t)\n",
    "\t\tassert len(att.shape) == 3, str(att.shape)\n",
    "\n",
    "\t\t# output projection\n",
    "\t\toutput = self.W_O(att)\n",
    "\t\tassert output.shape == x.shape, str(output.shape)\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer is made from transformer blocks, which include multi-headed attention, an MLP, and some LayerNorms in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# layernorm, attention, another layernorm, mlp\n",
    "\t\tself.ln_1: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.attention: nn.Module = MultiHeadedAttention(config)\n",
    "\t\tself.ln_2: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.mlp: nn.Module = nn.Sequential(\n",
    "\t\t\tnn.Linear(config.d_model, config.mlp_expansion * config.d_model),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(config.mlp_expansion * config.d_model, config.d_model),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tz = x + self.attention(self.ln_1(x))\n",
    "\t\treturn z + self.mlp(self.ln_2(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it all together to form the itty-bitty GPT. The transformer decomposes a prompt into token embeddings and positional encodings, which then go through a series of transformer blocks (number defined by `n_blocks`). This then goes through a final LayerNorm, and a linear de-embedding. \n",
    "\n",
    "What we get at the end of this process is a probability distribution over tokens, and thus we need to sample from this with some randomness (`temperature`) and convert those tokens into words to get an output. \n",
    "\n",
    "We will just use the GPT2 tokenizer for the sake of simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig, tokenizer: transformers.PreTrainedTokenizer):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.config: GPTConfig = config\n",
    "\t\tself.tokenizer: transformers.PreTrainedTokenizer = tokenizer\n",
    "\t\tassert config.d_vocab >= tokenizer.vocab_size\n",
    "\n",
    "\t\t# token and positional embeddings\n",
    "\t\tself.token_embeddings: nn.Module = nn.Embedding(config.d_vocab, config.d_model)\n",
    "\t\tself.positional_embeddings: nn.Module = nn.Embedding(config.n_context, config.d_model)\n",
    "\n",
    "\t\t# transformer\n",
    "\t\tself.transformer_blocks: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(config) \n",
    "\t\t\tfor _ in range(config.n_blocks)\n",
    "\t\t])\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tself.ln_f: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.lm_head: nn.Module = nn.Linear(config.d_model, config.d_vocab, bias=False)\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tx: Int[torch.Tensor, \"batch n_ctx\"],\n",
    "\t\t\ttargets: Int[torch.Tensor, \"batch n_ctx\"]|None = None,\n",
    "\t\t) -> tuple:\n",
    "\t\t\"\"\"returns a tuple of (logits, loss) where loss=None if targets is None\"\"\"\n",
    "\t\tassert x.ndim == 2, str(x.shape)\n",
    "\n",
    "\t\t# calculate token and positional embeddings and sum them\n",
    "\t\tx_res: Float[torch.Tensor, \"batch n_ctx d_model\"] = self.token_embeddings(x) + self.positional_embeddings(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "\t\tassert x_res.ndim == 3, str(x.shape)\n",
    "\n",
    "\t\t# transformer blocks\n",
    "\t\tfor i, block in enumerate(self.transformer_blocks):\n",
    "\t\t\tx_res = block(x_res)\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tlogits: Float[torch.Tensor, \"batch n_ctx d_vocab\"] = self.lm_head(self.ln_f(x_res))\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif targets is not None:\n",
    "\t\t\tloss = F.cross_entropy(\n",
    "\t\t\t\tlogits.transpose(1, 2),\n",
    "\t\t\t\ttargets,\n",
    "\t\t\t\tignore_index=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef generate(\n",
    "\t\tself,\n",
    "\t\tprompt: str|list[int]|Int[torch.Tensor, \"* n_ctx\"],\n",
    "\t\tmax_new_tokens: int = 128,\n",
    "\t\ttemperature: float = 1.0,\n",
    "\t) -> str:\n",
    "\n",
    "\t\t# convert prompt to string and tensor versions\n",
    "\t\tprompt_str: str\n",
    "\t\tprompt_tensor: Int[torch.Tensor, \"1 n_ctx\"]\n",
    "\t\tif isinstance(prompt, str):\n",
    "\t\t\tprompt_str = prompt\n",
    "\t\t\tprompt_tensor = torch.tensor(self.tokenizer.encode(prompt_str), dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, list):\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt)\n",
    "\t\t\tprompt_tensor = torch.tensor(prompt, dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, torch.Tensor):\n",
    "\t\t\tif prompt.ndim == 1:\n",
    "\t\t\t\tprompt = prompt.unsqueeze(0) # add batch dim\n",
    "\t\t\tassert prompt.ndim == 2\n",
    "\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt[0].tolist())\n",
    "\t\t\tprompt_tensor = prompt\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"prompt must be a string, list of ints, or PyTorch tensor\")\n",
    "\t\t\n",
    "\t\t# check tensor dims\n",
    "\t\tassert isinstance(prompt_str, str) \n",
    "\t\tassert isinstance(prompt_tensor, torch.Tensor)\n",
    "\t\tassert prompt_tensor.ndim == 2 \n",
    "\t\tassert prompt_tensor.shape[0] == 1\n",
    "\n",
    "\t\t#  device\n",
    "\t\tprompt_tensor = prompt_tensor.to(self.device)\n",
    "\n",
    "\t\t# pad the prompt if necessary\n",
    "\t\tif prompt_tensor.shape[1] < self.config.n_context:\n",
    "\t\t\tprompt_tensor = F.pad(prompt_tensor, (0, self.config.n_context - prompt_tensor.shape[1]), value=self.tokenizer.pad_token_id)\n",
    "\n",
    "\t\tassert prompt_tensor.shape[1] == self.config.n_context\n",
    "\n",
    "\t\t# iterate until max_new_tokens is reached, or an end-of-sequence token is generated\n",
    "\t\tcompletions: list[int] = list()\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\t# truncate sequence to block size\n",
    "\t\t\tprompt_len: int = prompt_tensor.shape[1]\n",
    "\t\t\tif prompt_len > self.config.n_context:\n",
    "\t\t\t\tprompt_tensor = prompt_tensor[:, -self.config.n_context:]\n",
    "\n",
    "\t\t\t# forward the model to get the logits for the index in the sequence\n",
    "\t\t\tlogits, _ = self(prompt_tensor)\n",
    "\n",
    "\t\t\t# pluck the logits at the final step and scale by desired temperature\n",
    "\t\t\tlogits = logits[:, -1, :] / temperature\n",
    "\n",
    "\t\t\t# apply softmax to convert logits to (normalized) probabilities\n",
    "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
    "\n",
    "\t\t\t# sample from the distribution\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\t\t\t# append sampled index to the running sequence and continue\n",
    "\t\t\tidx = torch.cat((prompt_tensor, idx_next), dim=1)\n",
    "\n",
    "\t\t\t# append the token to the running completions\n",
    "\t\t\tcompletions.append(int(idx_next[0, 0]))\n",
    "\n",
    "\t\t\t# check if end of sequence token is generated\n",
    "\t\t\tif idx_next == self.tokenizer.eos_token_id:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.tokenizer.decode(completions)\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\treturn sum(p.numel() for p in self.parameters())\n",
    "\t\n",
    "\t@property\n",
    "\tdef device(self) -> torch.device:\n",
    "\t\tdevice_set: set[torch.device] = set(p.device for p in self.parameters())\n",
    "\t\tassert len(device_set) == 1, device_set\n",
    "\t\treturn next(iter(device_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a full model to take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_cfg.n_params = 163009536\n",
      "test_model.n_params = 163037184\n",
      "GPT(\n",
      "  (token_embeddings): Embedding(50257, 768)\n",
      "  (positional_embeddings): Embedding(1024, 768)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_cfg = GPTConfig()\n",
    "print(f\"{test_cfg.n_params = }\")\n",
    "test_model = GPT(test_cfg, transformers.GPT2Tokenizer.from_pretrained(\"gpt2\"))\n",
    "print(f\"{test_model.n_params = }\")\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's use the TinyStories dataset, a well known dataset that gained fame when small yet still coherent models were trained on it. The dataset is made from a bunch of GPT generated children's stories, thus it does not have much diversity in content and should theoretically be pretty easy to learn. \n",
    "\n",
    "We only use a small chunk of the data for the sake of making training easy on a laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample story (story #8):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a peaceful town, there lived a little boy named Tim. Tim loved to run and play outside. One day, Tim saw a race in the park. He was excited and wanted to join the race.\\n\\nTim went to his friend, Sarah, and said, \"Let\\'s start the race!\" Sarah smiled and said, \"Yes, let\\'s go!\" They lined up with the other kids and waited for the race to begin. When they heard the word \"Go!\", they started running as fast as they could.\\n\\nTim and Sarah ran with all their speed, laughing and having fun. They could feel the wind in their hair as they raced to the finish line. In the end, Tim won the race and Sarah came in second. They were both so happy and proud of themselves. They celebrated with their friends and had a great day at the park.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grabbing the whole dataset\n",
    "text_data = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "#let's only use the training data\n",
    "text_data = text_data[\"train\"]\n",
    "\n",
    "# and let's only use the first 1000 stories \n",
    "text_data = text_data[:100]\n",
    "\n",
    "#what does a story look like? \n",
    "print(\"\\n Sample story (story #8):\")\n",
    "text_data['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's turn our dataset into a big long list of strings, and check how long it is (we want this to be small, millions or less): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76060"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = \"\\n\\n\".join(text_data['text'])\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: If you ever think this code isn't working, and want to do a sanity check, train the model on this dataset (which is just the letter \"a\" a bunch of times). When prompted with \"a\", your model output should feature tons of \"a\"s. You can use this as a sanity check (purely hypothetical, I definitely didn't do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a text dataset like text_data but filled strictly with \"a\" characters\n",
    "# text_data_a = \"a\" * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our TextDataset class, which is slightly inefficient because we have a fixed context size which we will stick with, and then we will trim off any data that isn't divisible by the context size. Ex: if we had 110 items in our dataset, and the context window is 25, it will chop off the last 10. Again, far from the best way, but it works and that's what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\ttext: str, \n",
    "\t\t\ttokenizer: transformers.PreTrainedTokenizer,\n",
    "\t\t\tn_context: int,\n",
    "\t\t\tensure_n_context_match: bool = True,\n",
    "\t\t):\n",
    "\t\t# add 1 to n_context to account for the target token\n",
    "\t\tn_context += 1\n",
    "\n",
    "\t\t# tokenize the text\n",
    "\t\ttokenized_text: list[int] = tokenizer.encode(text)\n",
    "\t\tself.total_tokens: int = len(tokenized_text)\n",
    "\n",
    "\t\t# trim the last tokens to make sure the length is a multiple of n_context\n",
    "\t\tif ensure_n_context_match:\n",
    "\t\t\ttokenized_text = tokenized_text[:-(len(tokenized_text) % n_context)]\n",
    "\t\t\tself.total_tokens = len(tokenized_text)\n",
    "\n",
    "\t\t# split the text into examples of length n_context\n",
    "\t\t# this means that text will often start in the middle of a sentence\n",
    "\t\t# in reality, we might want to do this a bit smarter\n",
    "\t\tself.examples: list[list[int]] = [\n",
    "\t\t\ttokenized_text[i:i+n_context] \n",
    "\t\t\tfor i in range(0, len(tokenized_text), n_context)\n",
    "\t\t]\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.examples)\n",
    "\t\n",
    "\tdef __getitem__(self, i: int) -> Float[torch.Tensor, \"n_ctx\"]:\n",
    "\t\treturn torch.tensor(self.examples[i], dtype=torch.long, device = 'cpu')\n",
    "\t\n",
    "\tdef example_lengths(self) -> Counter[int]:\n",
    "\t\treturn Counter(len(ex) for ex in self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Here we train the model! First, we define our training loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\tmodel: GPT,\n",
    "\ttext: str,\n",
    "\toptimizer: torch.optim.Optimizer,\n",
    "\tscheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "\tdevice: torch.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size: int = 8,\n",
    "\tmax_batches: int|None = None,\n",
    "\tprint_interval: int = 100,\n",
    "\tepochs: int = 1,\n",
    ") -> tuple[GPT, list[dict]]:\n",
    "\t\n",
    "\t# move model to device\n",
    "\tprint(f\"moving model to device: {device}\")\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t# set up data\n",
    "\tprint(f\"setting up dataset from text of length {len(text)}\")\n",
    "\tdataset: TextDataset = TextDataset(\n",
    "\t\ttext=text, \n",
    "\t\ttokenizer=model.tokenizer, \n",
    "\t\tn_context=model.config.n_context,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataset with {len(dataset)} examples, example lengths: {dataset.example_lengths()}\")\n",
    "\n",
    "\tprint(f\"setting up dataloader from {len(dataset)} examples\")\n",
    "\tdataloader: DataLoader = DataLoader(\n",
    "\t\tdataset, \n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tshuffle=True,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataloader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "\t# set up training loop\n",
    "\tprint(\"training...\")\n",
    "\ttraining_records: list[dict] = list()\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}\\n\")\n",
    "\t\ti: int; batch: Float[torch.Tensor, \"batch n_ctx\"]\n",
    "\t\tfor i, batch in tqdm.tqdm(\n",
    "\t\t\tenumerate(dataloader),\n",
    "\t\t\ttotal=len(dataloader),\n",
    "\t\t\tdesc=\"Training\",\n",
    "\t\t):\n",
    "\t\t\t# move batch to device\n",
    "\t\t\tbatch = batch.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# break if we've reached the maximum number of batches\n",
    "\t\t\tif max_batches is not None and i > max_batches:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# forward pass\n",
    "\t\t\tlogits, loss = model(\n",
    "\t\t\t\tbatch[:, :-1],\n",
    "\t\t\t\ttargets=batch[:, 1:], # the targets are just the input, offset by one\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# backward pass\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# record progress\n",
    "\t\t\ttraining_records.append({\n",
    "\t\t\t\t\"batch\": i,\n",
    "\t\t\t\t\"loss\": loss.item(),\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\tif i % print_interval == 0:\n",
    "\t\t\t\tprint(f\"Batch {i}, Loss: {loss.item()}\\n\")\n",
    "\n",
    "\t\tscheduler.step()\n",
    "    \t#print(f\"Updated learning rate to: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "\treturn model, training_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure (define params for) our own model (which will be tiny) and do some setup before we train it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER.vocab_size = 50257 \n",
      "\n",
      "Muutils rounded model params: \n",
      "MODEL.n_params = 807K \n",
      "\n",
      "Full model params: \n",
      "MODEL.n_params = 806896\n"
     ]
    }
   ],
   "source": [
    "# using the GPT2 tokenizer, and making sure it has the same vocab size as the model\n",
    "TOKENIZER: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"{TOKENIZER.vocab_size = } \\n\")\n",
    "\n",
    "\n",
    "# set up a config for a small model\n",
    "CONFIG: GPTConfig = GPTConfig(\n",
    "\td_model=8,\n",
    "\td_vocab=50257,\n",
    "\tn_context=128,\n",
    "\tn_blocks=2,\n",
    "\tn_head=4,\n",
    ")\n",
    "\n",
    "# not the most necessary check but it felt good to do\n",
    "assert(TOKENIZER.vocab_size == GPTConfig().d_vocab)\n",
    "\n",
    "# initialize the model\n",
    "MODEL: GPT = GPT(CONFIG, TOKENIZER)\n",
    "\n",
    "#two ways of printing number of model params\n",
    "print(\"Muutils rounded model params: \")\n",
    "print(f\"MODEL.n_params = {shorten_numerical_to_str(MODEL.n_params)} \\n\")\n",
    "print(\"Full model params: \")\n",
    "print(f\"MODEL.n_params = {MODEL.n_params}\")\n",
    "\n",
    "# choice of optimizer\n",
    "OPTIMIZER: torch.optim.Optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-1)\n",
    "#OPTIMIZER: torch.optim.Optimizer = torch.optim.SGD(MODEL.parameters(), lr=1e-1)\n",
    "# Initialize the learning rate scheduler\n",
    "SCHEDULER: StepLR = StepLR(OPTIMIZER, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving model to device: cuda"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18985 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "setting up dataset from text of length 76060\n",
      "\tset up dataset with 147 examples, example lengths: Counter({129: 147})\n",
      "setting up dataloader from 147 examples\n",
      "\tset up dataloader with 5 batches of size 32\n",
      "training...\n",
      "Epoch 1/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:02<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 10.996191024780273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:02<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.412898540496826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.077994346618652\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.8305253982543945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.740299224853516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.732051372528076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.609669208526611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.586735725402832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.414438247680664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.275589466094971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.175333023071289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.076552391052246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.960487365722656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.794473171234131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.612510681152344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.456562519073486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.2682695388793945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.247232437133789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.007120132446289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.919843912124634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.828779935836792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.771991014480591\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.679422616958618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.7336418628692627\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.666445732116699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.6640830039978027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.5375072956085205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.56088924407959\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4465444087982178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4558656215667725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.354921579360962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.4253392219543457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.301480531692505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.388551950454712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2704405784606934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3785653114318848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.289022445678711\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3116445541381836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.333580732345581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3421921730041504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.300762176513672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.31797456741333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3032357692718506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.252319574356079\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3085992336273193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.312325954437256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.176494836807251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2706167697906494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2670674324035645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2813639640808105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3265271186828613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2574334144592285\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.3307693004608154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.290410280227661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2459568977355957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2642757892608643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.313551902770996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2170932292938232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2072784900665283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.263341188430786\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2539002895355225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.262500047683716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.250119686126709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.211700677871704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2415099143981934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2066831588745117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2315480709075928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2262871265411377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2426071166992188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1837775707244873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2898035049438477\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1805989742279053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.226116180419922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.213773727416992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.182772397994995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2330636978149414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.266331434249878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2715206146240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:00<00:00,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1940135955810547\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.230990171432495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1858527660369873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2637217044830322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1614372730255127\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:01<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2729625701904297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.23771595954895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2109901905059814\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2800230979919434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2131099700927734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.207932710647583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:04<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1989264488220215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2044517993927\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.275451898574829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:02<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1337411403656006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2047863006591797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2032902240753174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:01,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.19120717048645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.237370252609253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.161142587661743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:01,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2532641887664795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2396087646484375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1478664875030518\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.186497449874878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:01<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:01<00:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.244889974594116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1625964641571045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.264606475830078\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:01<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:01<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.250946044921875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2210476398468018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1989755630493164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:02<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:01,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2543392181396484\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:00<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.210721731185913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.298192024230957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.219848871231079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:00<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1833271980285645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2257080078125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:01,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.220615863800049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:01<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.234238386154175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:00<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.196181058883667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.226818799972534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:01<00:04,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.1815834045410156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.2873728275299072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINED, training_history = train(\n",
    "\tmodel=MODEL,\n",
    "\ttext=text_data,\n",
    "\toptimizer=OPTIMIZER,\n",
    "    scheduler = SCHEDULER,\n",
    "\tdevice=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size=32,\n",
    "\tmax_batches=None,\n",
    "\tprint_interval=100,\n",
    "\tepochs=120,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save our trained model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MODEL_TRAINED, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code for loading the model back in: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model\n",
    "\n",
    "First, let's take a quick look at our loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#plot loss over epochs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m losses \u001b[38;5;241m=\u001b[39m [record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining_history\u001b[49m]\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_history' is not defined"
     ]
    }
   ],
   "source": [
    "#plot loss over epochs\n",
    "losses = [record[\"loss\"] for record in training_history]\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that it shakily decreased throughout the training run. Now let's test out some prompts, and see what our model gives us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_TRAINED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mMODEL_TRAINED\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time, Tim climbed\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_TRAINED' is not defined"
     ]
    }
   ],
   "source": [
    "print(MODEL_TRAINED.generate(\"Once upon a time, Tim climbed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but could be much worse.. We'll come back to this and make it actually work. I'm pretty sure the model we are using is just a bit less than the smallest TinyStories model (1M), so I assume we can pull this off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
