{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IttyBittyGPT \n",
    "\n",
    "Author: Antony Sikorski\n",
    "\n",
    "With lots of help and inspiration from Misha Ivanitsky and his LLM & Interpretability course, and Karpathy's NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we walk through:\n",
    "- Building the model\n",
    "- Setting up the dataset (we use TinyStories)\n",
    "- Training\n",
    "- and a bit of Prompting and analysis\n",
    "\n",
    "This is far from the most effective available implementation, and there are a number of things that could be improved, but I have found this to be the most effective way to learn how one works.\n",
    "\n",
    "This file has everything you need for cooking up your own little transformer and training it on a chunk of the TinyStories dataset, but doing everything in one Jupyter Notebook is not best practice. The other files in the repo are a more modular and effective way of splitting this notebook up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python_3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# necessary libraries \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from jaxtyping import Int, Float\n",
    "import tqdm\n",
    "import transformers\n",
    "import transformer_lens\n",
    "\n",
    "from muutils.misc import shorten_numerical_to_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if you have a GPU on your computer that you can run this on. That could make this process significantly faster, but you could also run out of memory (Cuda Out Of Memory error). If you don't have torch with CUDA, don't worry about this, you can just use your CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available() == True):\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use my laptop GPU, which is good news! Although I believe it only has 6 GB available, and only 1.5 of them will be available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary auto-reload for development on local machine\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "We first start off with a config class, which allows us to specify the params and size of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class GPTConfig:\n",
    "    \"\"\"Here we configure the dimensions of\n",
    "    our model. We'll set the defaults as the \n",
    "     dims from GPT2 \"\"\"\n",
    "    d_model: int = 768 # dimension of residual stream, the vectors it internally passes around \n",
    "    d_vocab: int = 50257 # defines the number of different tokens that can be represented as inputs (vocabulary size)\n",
    "    n_context: int = 1024 # maximum sequence length (context window size)\n",
    "    n_blocks: int = 12 # number of transformer blocks, frequently called n_layers but I don't like that \n",
    "    n_head: int = 12 # number of attention heads \n",
    "    head_bias: bool = True # whether to use bias in attention heads\n",
    "    mlp_expansion: int = 4 # expansion factor in MLP (they go from small to big to small, this is how many times bigger the middle layer is)\n",
    "\n",
    "    # model dimension must be divisible by number of heads\n",
    "    @property\n",
    "    def d_head(self):\n",
    "        assert self.d_model % self.n_head == 0, f\"'{self.d_model = }' must be divisible by '{self.n_head = }': {self.d_model} % {self.n_head} == {self.d_model % self.n_head}\"\n",
    "        return self.d_model // self.n_head\n",
    "    \n",
    "    @property\n",
    "    def params_shapes(self) -> dict:\n",
    "        return dict(\n",
    "            token_embeddings=(self.d_vocab, self.d_model),\n",
    "            positional_embeddings=(self.n_context, self.d_model),\n",
    "            attention_weights=(\n",
    "                self.n_blocks,\n",
    "                4,\n",
    "                self.d_model,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            attention_bias=(\n",
    "                self.n_blocks,\n",
    "                int(self.head_bias),\n",
    "                self.d_model,\n",
    "            ),\n",
    "            mlp_weights=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                self.d_model,\n",
    "                self.d_model * self.mlp_expansion,\n",
    "            ),\n",
    "            mlp_bias=(\n",
    "                self.n_blocks,\n",
    "                self.mlp_expansion + 1,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            block_layernorms=(\n",
    "                self.n_blocks,\n",
    "                2,\n",
    "                2,\n",
    "                self.d_model,\n",
    "            ),\n",
    "            output_layernorm=(2, self.d_model),\n",
    "            lm_head=(self.d_model, self.d_vocab),\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def params_numel(self) -> dict:\n",
    "        return {\n",
    "            k: int(torch.tensor(v).prod())\n",
    "            for k, v in self.params_shapes.items()\n",
    "        }\n",
    "\n",
    "    # will return the total number of parameters in the model\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return sum([v for v in self.params_numel.values()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separately define the attention mechanism, which is the defining component of the transformer architecture, because it allows the model to preserve long range dependencies by learning what to pay *attention* to. \n",
    "\n",
    "It is essentially some matrix multiplications with a soft-max and a causal mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # store dimensions\n",
    "        self.n_head: int = config.n_head\n",
    "        self.d_model: int = config.d_model\n",
    "        self.n_context: int = config.n_context\n",
    "\n",
    "        # concatenating the outputs of the heads should give us d_model, but this check is done in GPTConfig\n",
    "        self.d_head: int = config.d_head\n",
    "        self.head_bias: bool = config.head_bias\n",
    "\n",
    "        # magic coefficient for scaling the dot product of the query and key in the attention calculation\n",
    "        self.sqrt_dim: float = 1.0 / math.sqrt(self.d_head)\n",
    "    \n",
    "\n",
    "        # key, query, value projections\n",
    "        self.W_K: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_Q: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "        self.W_V: nn.Module = nn.Linear(self.d_model, self.d_head, bias = self.head_bias)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        # `register_buffer` means it's not a trainable parameter\n",
    "        # the point here is to not allow the model to \"look into the future\" when making predictions\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", \n",
    "            torch.tril(\n",
    "                torch.ones(config.n_context, config.n_context)\n",
    "            )\n",
    "            .view(1, 1, config.n_context, config.n_context)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_head\"]:\n",
    "        assert x.ndim == 3, str(x.shape)\n",
    "        B, n_ctx, d_model = x.shape # batch size, sequence length, embedding dimensionality (d_model)\n",
    "        assert d_model == self.d_model, str(x.shape)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_Q(x)\n",
    "        k: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_K(x)\n",
    "        v: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_V(x)\n",
    "\n",
    "        # self-attention\n",
    "        att = (q @ k.transpose(-2, -1)) * self.sqrt_dim\n",
    "        \n",
    "        # autoregressive (causal) masking\n",
    "        att = att.masked_fill(\n",
    "            self.causal_mask[:,:n_ctx,:n_ctx] == 0, \n",
    "            float('-inf'),\n",
    "        )\n",
    "\n",
    "        # softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # apply the self-attention to the values\n",
    "        output = att @ v\n",
    "        return output.view(B, n_ctx, self.d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are matrices that need to be learned, and we should check that the dims make sense: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead(\n",
      "  (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "  (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "A: AttentionHead = AttentionHead(GPTConfig())\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual demonstration of the causal mask. Although the plot you see is a matrix of $\\{0, 1\\}$ and not $\\{-\\infty, 0\\}$, we use `masked_fill` in the  `.forward()` function to set the elements of `attn` to $-\\infty$ where $M$ is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(A.causal_mask[0, 0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the multi-headed attention component. We compute the attention heads sequentially and then concatenate them, although it is very inefficient. Practical implementations do this all at once with one matrix multiplication, but the indexing for that is difficult to read and harder to learn from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_head: int = config.n_head\n",
    "\t\tself.d_head: int = config.d_model // config.n_head\n",
    "\t\tself.d_model: int = config.d_model\n",
    "\n",
    "\t\t# attention heads from previous class\n",
    "\t\tself.attention_heads: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tAttentionHead(config) \n",
    "\t\t\tfor _ in range(self.n_head)\n",
    "\t\t])\n",
    "\n",
    "\t\t# output projection\n",
    "\t\tself.W_O: nn.Module = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\t# apply all attention heads and concatenate their outputs\n",
    "\t\t# note: in reality, you would do this all in one tensor\n",
    "\t\t# we split the attention heads up to make it easier to understand\n",
    "\t\tatt = torch.cat(\n",
    "\t\t\t[\n",
    "\t\t\t\thead(x) \n",
    "\t\t\t\tfor head in self.attention_heads\n",
    "\t\t\t],\n",
    "\t\t\tdim=-1,\n",
    "\t\t)\n",
    "\t\tassert len(att.shape) == 3, str(att.shape)\n",
    "\n",
    "\t\t# output projection\n",
    "\t\toutput = self.W_O(att)\n",
    "\t\tassert output.shape == x.shape, str(output.shape)\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer is made from transformer blocks, which include multi-headed attention, an MLP, and some LayerNorms in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# layernorm, attention, another layernorm, mlp\n",
    "\t\tself.ln_1: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.attention: nn.Module = MultiHeadedAttention(config)\n",
    "\t\tself.ln_2: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.mlp: nn.Module = nn.Sequential(\n",
    "\t\t\tnn.Linear(config.d_model, config.mlp_expansion * config.d_model),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(config.mlp_expansion * config.d_model, config.d_model),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tz = x + self.attention(self.ln_1(x))\n",
    "\t\treturn z + self.mlp(self.ln_2(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it all together to form the itty-bitty GPT. The transformer decomposes a prompt into token embeddings and positional encodings, which then go through a series of transformer blocks (number defined by `n_blocks`). This then goes through a final LayerNorm, and a linear de-embedding. \n",
    "\n",
    "What we get at the end of this process is a probability distribution over tokens, and thus we need to sample from this with some randomness (`temperature`) and convert those tokens into words to get an output. \n",
    "\n",
    "We will just use the GPT2 tokenizer for the sake of simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig, tokenizer: transformers.PreTrainedTokenizer):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.config: GPTConfig = config\n",
    "\t\tself.tokenizer: transformers.PreTrainedTokenizer = tokenizer\n",
    "\t\tassert config.d_vocab >= tokenizer.vocab_size\n",
    "\n",
    "\t\t# token and positional embeddings\n",
    "\t\tself.token_embeddings: nn.Module = nn.Embedding(config.d_vocab, config.d_model)\n",
    "\t\tself.positional_embeddings: nn.Module = nn.Embedding(config.n_context, config.d_model)\n",
    "\n",
    "\t\t# transformer\n",
    "\t\tself.transformer_blocks: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(config) \n",
    "\t\t\tfor _ in range(config.n_blocks)\n",
    "\t\t])\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tself.ln_f: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.lm_head: nn.Module = nn.Linear(config.d_model, config.d_vocab, bias=False)\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tx: Int[torch.Tensor, \"batch n_ctx\"],\n",
    "\t\t\ttargets: Int[torch.Tensor, \"batch n_ctx\"]|None = None,\n",
    "\t\t) -> tuple:\n",
    "\t\t\"\"\"returns a tuple of (logits, loss) where loss=None if targets is None\"\"\"\n",
    "\t\tassert x.ndim == 2, str(x.shape)\n",
    "\n",
    "\t\t# calculate token and positional embeddings and sum them\n",
    "\t\tx_res: Float[torch.Tensor, \"batch n_ctx d_model\"] = self.token_embeddings(x) + self.positional_embeddings(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "\t\tassert x_res.ndim == 3, str(x.shape)\n",
    "\n",
    "\t\t# transformer blocks\n",
    "\t\tfor i, block in enumerate(self.transformer_blocks):\n",
    "\t\t\tx_res = block(x_res)\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tlogits: Float[torch.Tensor, \"batch n_ctx d_vocab\"] = self.lm_head(self.ln_f(x_res))\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif targets is not None:\n",
    "\t\t\tloss = F.cross_entropy(\n",
    "\t\t\t\tlogits.transpose(1, 2),\n",
    "\t\t\t\ttargets,\n",
    "\t\t\t\tignore_index=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef generate(\n",
    "\t\tself,\n",
    "\t\tprompt: str|list[int]|Int[torch.Tensor, \"* n_ctx\"],\n",
    "\t\tmax_new_tokens: int = 128,\n",
    "\t\ttemperature: float = 1.0,\n",
    "\t) -> str:\n",
    "\n",
    "\t\t# convert prompt to string and tensor versions\n",
    "\t\tprompt_str: str\n",
    "\t\tprompt_tensor: Int[torch.Tensor, \"1 n_ctx\"]\n",
    "\t\tif isinstance(prompt, str):\n",
    "\t\t\tprompt_str = prompt\n",
    "\t\t\tprompt_tensor = torch.tensor(self.tokenizer.encode(prompt_str), dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, list):\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt)\n",
    "\t\t\tprompt_tensor = torch.tensor(prompt, dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, torch.Tensor):\n",
    "\t\t\tif prompt.ndim == 1:\n",
    "\t\t\t\tprompt = prompt.unsqueeze(0) # add batch dim\n",
    "\t\t\tassert prompt.ndim == 2\n",
    "\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt[0].tolist())\n",
    "\t\t\tprompt_tensor = prompt\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"prompt must be a string, list of ints, or PyTorch tensor\")\n",
    "\t\t\n",
    "\t\t# check tensor dims\n",
    "\t\tassert isinstance(prompt_str, str) \n",
    "\t\tassert isinstance(prompt_tensor, torch.Tensor)\n",
    "\t\tassert prompt_tensor.ndim == 2 \n",
    "\t\tassert prompt_tensor.shape[0] == 1\n",
    "\n",
    "\t\t#  device\n",
    "\t\tprompt_tensor = prompt_tensor.to(self.device)\n",
    "\n",
    "\t\t# pad the prompt if necessary\n",
    "\t\tif prompt_tensor.shape[1] < self.config.n_context:\n",
    "\t\t\tprompt_tensor = F.pad(prompt_tensor, (0, self.config.n_context - prompt_tensor.shape[1]), value=self.tokenizer.pad_token_id)\n",
    "\n",
    "\t\tassert prompt_tensor.shape[1] == self.config.n_context\n",
    "\n",
    "\t\t# iterate until max_new_tokens is reached, or an end-of-sequence token is generated\n",
    "\t\tcompletions: list[int] = list()\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\t# truncate sequence to block size\n",
    "\t\t\tprompt_len: int = prompt_tensor.shape[1]\n",
    "\t\t\tif prompt_len > self.config.n_context:\n",
    "\t\t\t\tprompt_tensor = prompt_tensor[:, -self.config.n_context:]\n",
    "\n",
    "\t\t\t# forward the model to get the logits for the index in the sequence\n",
    "\t\t\tlogits, _ = self(prompt_tensor)\n",
    "\n",
    "\t\t\t# pluck the logits at the final step and scale by desired temperature\n",
    "\t\t\tlogits = logits[:, -1, :] / temperature\n",
    "\n",
    "\t\t\t# apply softmax to convert logits to (normalized) probabilities\n",
    "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
    "\n",
    "\t\t\t# sample from the distribution\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\t\t\t# append sampled index to the running sequence and continue\n",
    "\t\t\tidx = torch.cat((prompt_tensor, idx_next), dim=1)\n",
    "\n",
    "\t\t\t# append the token to the running completions\n",
    "\t\t\tcompletions.append(int(idx_next[0, 0]))\n",
    "\n",
    "\t\t\t# check if end of sequence token is generated\n",
    "\t\t\tif idx_next == self.tokenizer.eos_token_id:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.tokenizer.decode(completions)\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\treturn sum(p.numel() for p in self.parameters())\n",
    "\t\n",
    "\t@property\n",
    "\tdef device(self) -> torch.device:\n",
    "\t\tdevice_set: set[torch.device] = set(p.device for p in self.parameters())\n",
    "\t\tassert len(device_set) == 1, device_set\n",
    "\t\treturn next(iter(device_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a full model to take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_cfg.n_params = 163009536\n",
      "test_model.n_params = 163037184\n",
      "GPT(\n",
      "  (token_embeddings): Embedding(50257, 768)\n",
      "  (positional_embeddings): Embedding(1024, 768)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MultiHeadedAttention(\n",
      "        (attention_heads): ModuleList(\n",
      "          (0): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (3): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (4): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (5): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (6): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (7): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (8): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (9): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (10): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (11): AttentionHead(\n",
      "            (W_K): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_Q): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (W_V): Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (W_O): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_cfg = GPTConfig()\n",
    "print(f\"{test_cfg.n_params = }\")\n",
    "test_model = GPT(test_cfg, transformers.GPT2Tokenizer.from_pretrained(\"gpt2\"))\n",
    "print(f\"{test_model.n_params = }\")\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's use the TinyStories dataset, a well known dataset that gained fame when small yet still coherent models were trained on it. The dataset is made from a bunch of GPT generated children's stories, thus it does not have much diversity in content and should theoretically be pretty easy to learn. \n",
    "\n",
    "We only use a small chunk of the data for the sake of making training easy on a laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample story (story #8):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a peaceful town, there lived a little boy named Tim. Tim loved to run and play outside. One day, Tim saw a race in the park. He was excited and wanted to join the race.\\n\\nTim went to his friend, Sarah, and said, \"Let\\'s start the race!\" Sarah smiled and said, \"Yes, let\\'s go!\" They lined up with the other kids and waited for the race to begin. When they heard the word \"Go!\", they started running as fast as they could.\\n\\nTim and Sarah ran with all their speed, laughing and having fun. They could feel the wind in their hair as they raced to the finish line. In the end, Tim won the race and Sarah came in second. They were both so happy and proud of themselves. They celebrated with their friends and had a great day at the park.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grabbing the whole dataset\n",
    "text_data = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "#let's only use the training data\n",
    "text_data = text_data[\"train\"]\n",
    "\n",
    "# and let's only use the first 1000 stories \n",
    "text_data = text_data[:100]\n",
    "\n",
    "#what does a story look like? \n",
    "print(\"\\n Sample story (story #8):\")\n",
    "text_data['text'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's turn our dataset into a big long list of strings, and check how long it is (we want this to be small, millions or less): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76060"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = \"\\n\\n\".join(text_data['text'])\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: If you ever think this code isn't working, and want to do a sanity check, train the model on this dataset (which is just the letter \"a\" a bunch of times). When prompted with \"a\", your model output should feature tons of \"a\"s. You can use this as a sanity check (purely hypothetical, I definitely didn't do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a text dataset like text_data but filled strictly with \"a\" characters\n",
    "# text_data_a = \"a\" * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our TextDataset class, which is slightly inefficient because we have a fixed context size which we will stick with, and then we will trim off any data that isn't divisible by the context size. Ex: if we had 110 items in our dataset, and the context window is 25, it will chop off the last 10. Again, far from the best way, but it works and that's what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\ttext: str, \n",
    "\t\t\ttokenizer: transformers.PreTrainedTokenizer,\n",
    "\t\t\tn_context: int,\n",
    "\t\t\tensure_n_context_match: bool = True,\n",
    "\t\t):\n",
    "\t\t# add 1 to n_context to account for the target token\n",
    "\t\tn_context += 1\n",
    "\n",
    "\t\t# tokenize the text\n",
    "\t\ttokenized_text: list[int] = tokenizer.encode(text)\n",
    "\t\tself.total_tokens: int = len(tokenized_text)\n",
    "\n",
    "\t\t# trim the last tokens to make sure the length is a multiple of n_context\n",
    "\t\tif ensure_n_context_match:\n",
    "\t\t\ttokenized_text = tokenized_text[:-(len(tokenized_text) % n_context)]\n",
    "\t\t\tself.total_tokens = len(tokenized_text)\n",
    "\n",
    "\t\t# split the text into examples of length n_context\n",
    "\t\t# this means that text will often start in the middle of a sentence\n",
    "\t\t# in reality, we might want to do this a bit smarter\n",
    "\t\tself.examples: list[list[int]] = [\n",
    "\t\t\ttokenized_text[i:i+n_context] \n",
    "\t\t\tfor i in range(0, len(tokenized_text), n_context)\n",
    "\t\t]\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.examples)\n",
    "\t\n",
    "\tdef __getitem__(self, i: int) -> Float[torch.Tensor, \"n_ctx\"]:\n",
    "\t\treturn torch.tensor(self.examples[i], dtype=torch.long, device = 'cpu')\n",
    "\t\n",
    "\tdef example_lengths(self) -> Counter[int]:\n",
    "\t\treturn Counter(len(ex) for ex in self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Here we train the model! First, we define our training loop. Our dataset is saved on the CPU, but everything else will be on GPU. We will move our batches from CPU to GPU. This slows things down, but is worth the extra compute because it enables us to use more data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\tmodel: GPT,\n",
    "\ttext: str,\n",
    "\toptimizer: torch.optim.Optimizer,\n",
    "\tscheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "\tdevice: torch.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size: int = 8,\n",
    "\tmax_batches: int|None = None,\n",
    "\tprint_interval: int = 100,\n",
    "\tepochs: int = 1,\n",
    ") -> tuple[GPT, list[dict]]:\n",
    "\t\n",
    "\t# move model to device\n",
    "\tprint(f\"moving model to device: {device}\")\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t# set up data\n",
    "\tprint(f\"setting up dataset from text of length {len(text)}\")\n",
    "\tdataset: TextDataset = TextDataset(\n",
    "\t\ttext=text, \n",
    "\t\ttokenizer=model.tokenizer, \n",
    "\t\tn_context=model.config.n_context,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataset with {len(dataset)} examples, example lengths: {dataset.example_lengths()}\")\n",
    "\n",
    "\tprint(f\"setting up dataloader from {len(dataset)} examples\")\n",
    "\tdataloader: DataLoader = DataLoader(\n",
    "\t\tdataset, \n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tshuffle=True,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataloader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "\t# set up training loop\n",
    "\tprint(\"training...\")\n",
    "\ttraining_records: list[dict] = list()\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}\\n\")\n",
    "\t\ti: int; batch: Float[torch.Tensor, \"batch n_ctx\"]\n",
    "\t\tfor i, batch in tqdm.tqdm(\n",
    "\t\t\tenumerate(dataloader),\n",
    "\t\t\ttotal=len(dataloader),\n",
    "\t\t\tdesc=\"Training\",\n",
    "\t\t):\n",
    "\t\t\t# move batch to device \n",
    "\t\t\t# you have most likely saved the dataset to the cpu, so you need to move it to the gpu\n",
    "\t\t\tbatch = batch.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# break if we've reached the maximum number of batches\n",
    "\t\t\tif max_batches is not None and i > max_batches:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# forward pass\n",
    "\t\t\tlogits, loss = model(\n",
    "\t\t\t\tbatch[:, :-1],\n",
    "\t\t\t\ttargets=batch[:, 1:], # the targets are just the input, offset by one\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# backward pass\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# record progress\n",
    "\t\t\ttraining_records.append({\n",
    "\t\t\t\t\"batch\": i,\n",
    "\t\t\t\t\"loss\": loss.item(),\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\tif i % print_interval == 0:\n",
    "\t\t\t\tprint(f\"Batch {i}, Loss: {loss.item()}\\n\")\n",
    "\n",
    "\t\tscheduler.step()\n",
    "    \t#print(f\"Updated learning rate to: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "\treturn model, training_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure (define params for) our own model (which will be tiny) and do some setup before we train it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER.vocab_size = 50257 \n",
      "\n",
      "Muutils rounded model params: \n",
      "MODEL.n_params = 807K \n",
      "\n",
      "Full model params: \n",
      "MODEL.n_params = 806896\n"
     ]
    }
   ],
   "source": [
    "# using the GPT2 tokenizer, and making sure it has the same vocab size as the model\n",
    "TOKENIZER: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"{TOKENIZER.vocab_size = } \\n\")\n",
    "\n",
    "\n",
    "# set up a config for a small model\n",
    "CONFIG: GPTConfig = GPTConfig(\n",
    "\td_model=8,\n",
    "\td_vocab=50257,\n",
    "\tn_context=128,\n",
    "\tn_blocks=2,\n",
    "\tn_head=4,\n",
    ")\n",
    "\n",
    "# not the most necessary check but it felt good to do\n",
    "assert(TOKENIZER.vocab_size == GPTConfig().d_vocab)\n",
    "\n",
    "# initialize the model\n",
    "MODEL: GPT = GPT(CONFIG, TOKENIZER)\n",
    "\n",
    "#two ways of printing number of model params\n",
    "print(\"Muutils rounded model params: \")\n",
    "print(f\"MODEL.n_params = {shorten_numerical_to_str(MODEL.n_params)} \\n\")\n",
    "print(\"Full model params: \")\n",
    "print(f\"MODEL.n_params = {MODEL.n_params}\")\n",
    "\n",
    "# choice of optimizer\n",
    "OPTIMIZER: torch.optim.Optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-1)\n",
    "#OPTIMIZER: torch.optim.Optimizer = torch.optim.SGD(MODEL.parameters(), lr=1e-1)\n",
    "# Initialize the learning rate scheduler\n",
    "SCHEDULER: StepLR = StepLR(OPTIMIZER, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving model to device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18985 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up dataset from text of length 76060\n",
      "\tset up dataset with 147 examples, example lengths: Counter({129: 147})\n",
      "setting up dataloader from 147 examples\n",
      "\tset up dataloader with 5 batches of size 32\n",
      "training...\n",
      "Epoch 1/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|      | 2/5 [00:05<00:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 10.94760799407959\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:05<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.412278652191162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.863760471343994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.572959899902344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.32457160949707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.051607608795166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.910337924957275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.647175312042236\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.275420188903809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|        | 1/5 [00:00<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 4.257217884063721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5/5 [00:00<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINED, training_history = train(\n",
    "\tmodel=MODEL,\n",
    "\ttext=text_data,\n",
    "\toptimizer=OPTIMIZER,\n",
    "    scheduler = SCHEDULER,\n",
    "\tdevice=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size=32,\n",
    "\tmax_batches=None,\n",
    "\tprint_interval=100,\n",
    "\tepochs=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save our trained model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MODEL_TRAINED, \"C:/Users/anton/Desktop/Mines/DSCI _575_AdvML/Programming_Project/saved_models/tutorial_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model\n",
    "\n",
    "First, let's take a quick look at our loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss over epochs\n",
    "losses = [record[\"loss\"] for record in training_history]\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that it shakily decreased throughout the training run. Now let's test out some prompts, and see what our model gives us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Soeddy  It\n",
      "\n",
      " spot\n",
      "  Then\n",
      " He\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " She He Emily  They She They When They\n",
      " He They hands This\n",
      " The InThe case But It What\n",
      " They SheMary\n",
      "sola\n",
      " With ride One They  His Theywed car There\n",
      " The She\n",
      "\n",
      "\n",
      " She Lily car\n",
      "\n",
      "\n",
      "\n",
      " She It They The As Nora Ited\n",
      " He\n",
      "\n",
      " The He\n",
      "\n",
      "\n",
      "  \n",
      "- But But\n",
      " She\n",
      " SheBut She eye  toys He That He \n",
      "\n",
      "\n",
      " Oneeddy Inside He It dress One That The She rabbit course\n",
      " He She Nora She\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_TRAINED.generate(\"Once upon a time, Tim climbed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but could be much worse.. We'll come back to this and make it actually work. I'm pretty sure the model we are using is just a bit less than the smallest TinyStories model (1M), so I assume we can pull this off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
