{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting of Trained Model\n",
    "\n",
    "Author: Antony Sikorski\n",
    "\n",
    "In this notebook, we load the model and prompt it like we would any other chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from jaxtyping import Int, Float\n",
    "import tqdm\n",
    "import transformers\n",
    "import transformer_lens\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "#imports from files\n",
    "from text_dataset import TextDataset\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab our trained model (change the path to wherever you choose to save your models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TRAINED = GPT.read(\"C:/Users/anton/Desktop/Mines/DSCI _575_AdvML/Programming_Project/saved_models/model.zanj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your prompt as a string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, Tim climbed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The\n",
      "\n",
      " He She Benny The out This  He\n",
      " The The She\n",
      "�\n",
      " mom He Thatâ \n",
      " mom  Go The Sus  He He As This\n",
      "â The He  The Oneâ He The\n",
      "\n",
      " She Jack\n",
      "\n",
      "  The�\n",
      " The So The The\n",
      " He Let One He He ThisMary Heâ Tim She mom One\n",
      " The\n",
      " He  So The Rachel\n",
      " Timpaced Tim One The  The Tom The\n",
      " The They We The They\n",
      "\n",
      " The The He The The\n",
      "\n",
      " He coat\n",
      " The  They Fl\n",
      " Grace One  He  Let Heâ We He Tim Heâ full He\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_TRAINED.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but could be much worse.. We'll come back to this and make it actually work. I'm pretty sure the model we are using is just a bit less than the smallest TinyStories model (1M), so I assume we can pull this off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
