{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting of Trained Model\n",
    "\n",
    "Author: Antony Sikorski\n",
    "\n",
    "In this notebook, we load the model and prompt it like we would any other chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get any package error, install the requirements please: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from jaxtyping import Int, Float\n",
    "import tqdm\n",
    "import transformers\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "#imports from files\n",
    "from text_dataset import TextDataset\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab our trained model (change the path to wherever you choose to save your models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TRAINED = GPT.read(\"./saved_models/model.zanj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your prompt as a string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, Tim climbed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " It\n",
      " It\n",
      " The The\n",
      " The The\n",
      " It\n",
      " Tiny Inside She It\n",
      " It Its It\n",
      "\n",
      " It Can It\n",
      " \"\n",
      " It It She It\n",
      " It\n",
      " It The\n",
      " It\n",
      " The It Tom\n",
      " A It\n",
      " I Lily The She He Salt The It\n",
      " The\n",
      "\n",
      " It It It The She The Spot It \"\n",
      "\n",
      " He She Be Open\n",
      "\n",
      " She A It\n",
      " \n",
      "\n",
      "\n",
      " It It\n",
      " He It \" It\n",
      "\n",
      " The\n",
      "\n",
      " It It The\n",
      " It It It\n",
      " Sue He She He\n",
      " It It\n",
      " We In It\n",
      " Everything It\n",
      " \" It Sue It \"\n",
      "\n",
      " Finally\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_TRAINED.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but could be much worse.. We'll come back to this and make it actually work. I'm pretty sure the model we are using is just a bit less than the smallest TinyStories model (1M), so I assume we can pull this off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
